{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8d98iavoq3o"
   },
   "source": [
    "# Tensorflow Training in UDFs on GPUs\n",
    "\n",
    "**In this exampe, we show how to train a Tensorflow Model for Classification on the GPUs inside a UDF.**\n",
    "\n",
    "We use for this Example the Google Cloud for creating a VM with a GPU, because was the easiest way to the Demo up. This Demo should run on each other cloud providers, too, if you replace the the gcloud ans scripts commands with equivalent commands or scripts for a other cloud provider.\n",
    "\n",
    "The Demo works currently only with [Google Colaboratory](https://colab.research.google.com) which is Jupyter Notebook Service from Google, because it supports interactive commandline inputs. Which is with Jupyter currently not possible. This [article](https://research.google.com/colaboratory/local-runtimes.html) explains, how can connect your local Jupyter Server with colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dznZzdS-JSjY"
   },
   "source": [
    "## Start the Google Cloud Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLOwdyDUCnEX"
   },
   "source": [
    "First, you need to create the VM instance we want to use for this example and setup the environment. Before you can create the VM instance in the Google Cloud you need to login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHsh75o_-cAd"
   },
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwILxgvPDOFH"
   },
   "source": [
    "Next, you need to set the project, zone, VPC, name and the GPU type for the instance you want to create. The VPC need to allow ssh access on port 22 from outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-0sSm5x-pdx"
   },
   "outputs": [],
   "source": [
    "# Change the following Github Variables if you work on a other branch or fork\n",
    "GITHUB_USER=\"exasol\"\n",
    "GITHUB_BRANCH=\"master\"\n",
    "\n",
    "# Google Cloud Informations\n",
    "PROJECT_NAME=\"<PROJECT_NAME>\"\n",
    "INSTANCE_NAME=\"exasol-docker-db-gpu-demo\"\n",
    "VPC=\"default\"\n",
    "INSTANCE_ZONE=\"<ZONE>\" # Which GPUs are which zone avaialable: https://cloud.google.com/compute/docs/gpus/\n",
    "ACCELERATOR_TYPE=\"nvidia-tesla-k80\" # Comparable cheap and as such good for testing\n",
    "#ACCELERATOR_TYPE=\"nvidia-tesla-v100\"\n",
    "ACCELERATOR_COUNT=1\n",
    "!gcloud config set project $PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeteKT6gDqBe"
   },
   "source": [
    "Now, you can create or start the VM instance. We prepared a shell script which defines the gcloud command to create the VM instance. A setup script for the VM installs all necassary dependencies, such as the CUDA Driver, Docker or Nvidia Docker. After that, it starts a Exasol docker-db with Nvidia Docker. Nvidia Docker makes it possible to passthrough GPUs to Docker Container and in that case to the Exasol Database. This example is a preview, as such we currently don't provide solutions to prepare a ExaSolo or a Exasol Cluster for GPU Support. Furthermore, we are currently bound to a specific CUDA Driver and SDK Version, because Tensorflow gets installed into the Script Language Container via pip and these Versions of Tensorflow depend on specific versions of the CUDA Driver. We currently use the following versions:\n",
    "-Tensorflow 1.13.1\n",
    "- CUDA SDK 10\n",
    "- CUDA Driver 410.104 \n",
    "\n",
    "NOTE: This script starts a VM instance with a GPU. Instances with GPUs can be quite expensive, so stop or delete the instance after you finished the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7TmuB1g_Ebc"
   },
   "outputs": [],
   "source": [
    "!curl -o gcloud-create-instance.sh https://raw.githubusercontent.com/$GITHUB_USER/data-science-examples/$GITHUB_BRANCH/examples/tensorflow-with-gpu-preview/gcloud-create-instance.sh\n",
    "!chmod +x ./gcloud-create-instance.sh\n",
    "!./gcloud-create-instance.sh $INSTANCE_NAME --zone $INSTANCE_ZONE --accelerator=count=$ACCELERATOR_COUNT,type=$ACCELERATOR_TYPE --network $VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xp4q-EnPNOVB"
   },
   "source": [
    "With the following command you can start a stopped VM instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2ypsSA0NE6L"
   },
   "outputs": [],
   "source": [
    "!gcloud compute instances start $INSTANCE_NAME --zone $INSTANCE_ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmJ1VjigHBNF"
   },
   "source": [
    "After, the VM instance has started, we need to find its EXTERNAL IP and save it  in a variable, such that we can use it later to connect to the Exasol database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MGWZytqFZwn"
   },
   "outputs": [],
   "source": [
    "!gcloud compute instances list | grep \"$INSTANCE_NAME\" | grep \"$INSTANCE_ZONE\" | tee instance_info\n",
    "import re\n",
    "with open(\"instance_info\") as f:\n",
    "  instance_info_str = f.read()\n",
    "  columns=re.compile(\" +\").split(instance_info_str)\n",
    "  ips=[x for x in columns if re.compile(\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\").match(x)]\n",
    "  INSTANCE_IP=ips[1]\n",
    "  INTERNAL_INSTANCE_IP=ips[0]\n",
    "print(f\"External IP of VM instance is {INSTANCE_IP}\")\n",
    "print(f\"Internal IP of VM instance is {INTERNAL_INSTANCE_IP}\")\n",
    "CONNECTION_IP=INSTANCE_IP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IatNA2vKJtGE"
   },
   "source": [
    "## Notebook Runtime Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AVeEkfZpE9Yg"
   },
   "source": [
    "In the local notebook runtime we need the following package to connect to the Exasol Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmpGYyIf_pM3"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install tmux\n",
    "!pip install pyexasol stopwatch.py requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kejGJvOvLcI9"
   },
   "source": [
    "We connect us to the instance via SSH and forward the necassary ports for the Exasol Database to the Notebook Runtime. This allows us to connect to the database without opening ports in the VPC Firewall, because normally the ssh port is open and can be used to connect to the instance. The first connection needs to be done interactivily, to create the ssh key to the instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AafW0UV8RG-i"
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.ssh\n",
    "!echo \"\\n\\n\\n\" | gcloud compute ssh --zone \"$INSTANCE_ZONE\" \"$INSTANCE_NAME\" -- echo \"Connection succeded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7atEor4wRxbh"
   },
   "source": [
    "The actual port forwarding, we do in a non-interactive ssh session which we detach with tmux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Uab-Ubx7mKg"
   },
   "outputs": [],
   "source": [
    "!tmux kill-session -t sshtunel\n",
    "!tmux new -s sshtunel -d 'gcloud compute ssh --zone \"$INSTANCE_ZONE\" \"$INSTANCE_NAME\" -- -v -N -L8888:0.0.0.0:8888 -L6583:0.0.0.0:6583 &> ssh.log'\n",
    "CONNECTION_IP=\"127.0.0.1\"\n",
    "!tmux ls\n",
    "!sleep 5\n",
    "!cat ssh.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtBuzKG6IP2S"
   },
   "source": [
    "Now we can try to connect to the Exasol Database. \n",
    "\n",
    "NOTE: If it doesn't work with the first attempt, the database startup probably did not finished, just yet. Wait a few seconds/minutes and try a again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pofxkvV9IO0h"
   },
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "import textwrap\n",
    "import time\n",
    "from stopwatch import Stopwatch\n",
    "def connect(CONNECTION_IP, log=True):\n",
    "  dsn=f\"{CONNECTION_IP}:8888\"\n",
    "  if log:\n",
    "    print(f\"Connect to dsn {dsn}\")\n",
    "  c=pyexasol.connect(dsn=dsn,user=\"sys\",password=\"exasol\")\n",
    "  c.execute(\"CREATE SCHEMA IF NOT EXISTS test;\")\n",
    "  c.execute(\"OPEN SCHEMA test;\")\n",
    "  return c\n",
    "for i in range(20):\n",
    "  try:\n",
    "    c = connect(CONNECTION_IP)\n",
    "    break\n",
    "  except:\n",
    "    print(\"Connection attempt failed, will try again in 10 seconds\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6GSVjdPJ6so"
   },
   "source": [
    "## Setting up the Exasol Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XzRBQR-xiYe"
   },
   "source": [
    "After, you were able to connect to the database, we need to setup the database as self. First, we setup the UDF Output Redirect, to be able to get Standard Output and Standard Error from UDFs. This makes the debugging of problems with UDFs easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_08JO05iRFs"
   },
   "outputs": [],
   "source": [
    "!curl -o fetch_output_redirect_from_last_statement.sh https://raw.githubusercontent.com/$GITHUB_USER/data-science-examples/$GITHUB_BRANCH/examples/tensorflow-with-gpu-preview/fetch_output_redirect_from_last_statement.sh\n",
    "!gcloud compute ssh --zone \"$INSTANCE_ZONE\" \"$INSTANCE_NAME\" -- \"curl https://raw.githubusercontent.com/$GITHUB_USER/data-science-examples/$GITHUB_BRANCH/examples/tensorflow-with-gpu-preview/start_output_redirect_server.sh | bash\"\n",
    "c = connect(CONNECTION_IP)\n",
    "def set_script_output_address(c,INTERNAL_INSTANCE_IP):\n",
    "  c.execute(f\"ALTER SESSION SET SCRIPT_OUTPUT_ADDRESS = '{INTERNAL_INSTANCE_IP}:9999';\")\n",
    "  print(\"SCRIPT OUTPUT ADRESS was set\")\n",
    "set_script_output_address(c,INTERNAL_INSTANCE_IP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_K-qFhlNdPG"
   },
   "source": [
    "Next, we need to load the Script Language Container specifically build for using it  with Tensorflow on GPU's and CUDA. We use a  UDF to download the prepackaged Container directly to the database to avoid to temporary store it in the notebook runtime. The following UDF downloads a file from a given URL and uploads the content to a given BucketFS URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSprLSVaOi39"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "sql = textwrap.dedent(\"\"\"\n",
    "CREATE OR REPLACE PYTHON SET SCRIPT download_url_and_upload_to_bucketfs(download_url VARCHAR(2000000),upload_url VARCHAR(2000000)) \n",
    "EMITS(outputs VARCHAR(2000000)) AS \n",
    "def run(ctx):\n",
    "  import requests\n",
    "  download_url=ctx.download_url\n",
    "  upload_url=ctx.upload_url\n",
    "  r_download=requests.get(download_url,stream=True)\n",
    "  r_upload=requests.put(upload_url, data=r_download.iter_content(10*1024))\n",
    "  ctx.emit(str(r_upload.status_code))\n",
    "/\"\"\")\n",
    "c.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7OtM-ITyv1d"
   },
   "source": [
    "In the next step, we download the CUDA Preview Script Language Container from https://github.com/exasol/script-languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RoH2WMyhu4G"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "download_url='https://storage.googleapis.com/exasol-integration-demo/python3-ds-cuda-preview-EXASOL-6.1.0-release-OHJFKDQVKZYGSP7AWQWAYWBECT577SK6DG5JBR47Z2TL7GQ2M4OA.tar.gz'\n",
    "upload_url='http://w:write@localhost:6583/default/python3-ds-cuda-preview-EXASOL-6.1.0.tar.gz'\n",
    "s=c.execute(f\"\"\"select download_url_and_upload_to_bucketfs('{download_url}','{upload_url}')\"\"\")\n",
    "s.fetchall()\n",
    "!curl f\"w:write@{CONNECTION_IP}:6583/default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VLEJWaddynf"
   },
   "source": [
    "After the upload of the prepackaged Script Language Container is finished, we need define a new Script Language in the Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybdrbyrmlVjA"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "def add_python3_script_language(c):\n",
    "  s=c.execute(\"ALTER SESSION SET SCRIPT_LANGUAGES='JAVA=builtin_java PYTHON=builtin_python PYTHON3=localzmq+protobuf:///bfsdefault/default/python3-ds-cuda-preview-EXASOL-6.1.0?lang=python#buckets/bfsdefault/default/python3-ds-cuda-preview-EXASOL-6.1.0/exaudf/exaudfclient_py3';\")\n",
    "  print(\"Added PYTHON3 Script Language\")\n",
    "add_python3_script_language(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vV0JmmKZOmbY"
   },
   "source": [
    "We also create a utility UDF which shows the files in the BucketFS. This allows us to inspect the extracted content of the downloaded archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZDbxiEkiRc1"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "sql = textwrap.dedent(\"\"\"\n",
    "CREATE OR REPLACE PYTHON SET SCRIPT list_files(input_path VARCHAR(2000000)) \n",
    "EMITS(outputs VARCHAR(2000000)) AS \n",
    "def run(ctx):\n",
    "  import subprocess\n",
    "  result=subprocess.check_output(\"ls \"+ctx.input_path,shell=True)\n",
    "  for line in result.encode('utf-8').splitlines():\n",
    "    ctx.emit(line)\n",
    "/\"\"\")\n",
    "c.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9GqOIHHQtwk"
   },
   "source": [
    "The following query shows us the content of the pre-packaged python3-ds-cuda-preview-EXASOL-6.1.0  UDF Container in the BucketFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2ITOKbrqKw0"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "s=c.execute(\"select list_files('/buckets/bfsdefault/default/python3-ds-cuda-preview-EXASOL-6.1.0')\")\n",
    "print(s.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDTFWe9eP1Cg"
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGAkwqcGFyXG"
   },
   "source": [
    "We load the data from a public Google Storage Bucket via the CSV Import from a URL. As example dataset, we use the [Fine Food Reviews Dataset](https://snap.stanford.edu/data/web-FineFoods.html). It consists of 500000 reviews about Fine Food products on Amazon together with the ProductIDs, AuthorIDs, scores and helpfulness. We chose this dataset, because it contains columns with different characteristics. For example, ProductIDs and AuthorIDs are categorical data. The score or the helpfullness are numbers and the review is text data. Each of these types needs a different encoding strategy and especially text requries quite large models which benefit from GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "se5QLB36Lopg"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "c.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE test.fine_food_reviews (\n",
    "  ID INTEGER,\n",
    "  PRODUCTID VARCHAR(2000000),\n",
    "  USERID VARCHAR(2000000),\n",
    "  PROFILENAME VARCHAR(2000000),\n",
    "  HELPFULLNESSNUMERATOR INTEGER,\n",
    "  HELPFULLNESSDENOMINATOR INTEGER, \t\n",
    "  SCORE INTEGER, \t\n",
    "  UNIX_TIMESTAMP INTEGER, \t\n",
    "  SUMMARY VARCHAR(2000000),\n",
    "  TEXT VARCHAR(2000000));\n",
    "\"\"\")\n",
    "s=c.execute(\n",
    "\"\"\"\n",
    "IMPORT INTO test.fine_food_reviews\n",
    "FROM CSV AT 'https://storage.googleapis.com/exasol-integration-demo/' \n",
    "FILE 'fine_food_reviews.csv'\n",
    "ROW SEPARATOR = 'LF'\n",
    "COLUMN SEPARATOR = 'TAB'\n",
    "COLUMN DELIMITER = '\"'\n",
    "SKIP = 1;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B24bCrAPoeE"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "c.export_to_pandas(\"SELECT * FROM test.fine_food_reviews limit 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19dNKK2X2Ymk"
   },
   "source": [
    "## Training a Tensorflow Model on a GPU in a UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0_fD4FRyisv"
   },
   "source": [
    "After the import of the Dataset, you now need to download the Tensflow UDF Code into BucketFS from [Github](https://github.com/exasol/data-science-examples/tree/master/examples/tensorflow-with-gpu-preview). We later import this code into the actual UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VoxhoA3hsGc"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "TENSORFLOW_UDF_CODE_PATH_IN_BUCKET=f\"udf/code/{GITHUB_BRANCH}\"\n",
    "TENSORFLOW_UDF_CODE_BUCKETFS_PATH=f\"/buckets/bfsdefault/default/{TENSORFLOW_UDF_CODE_PATH_IN_BUCKET}\"\n",
    "download_url=f'https://github.com/{GITHUB_USER}/data-science-examples/archive/{GITHUB_BRANCH}.zip'\n",
    "upload_url=f'http://w:write@localhost:6583/default/{TENSORFLOW_UDF_CODE_PATH_IN_BUCKET}.zip'\n",
    "s=c.execute(f\"\"\"select download_url_and_upload_to_bucketfs('{download_url}','{upload_url}')\"\"\")\n",
    "s.fetchall()\n",
    "!curl f\"w:write@{CONNECTION_IP}:6583/default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJb6z09oOxPN"
   },
   "source": [
    "The following query shows you, the content of the Tensflow UDF Code Archive from Github which we uploaded to the BucketFS. If it failes, than the containers is not yet extracted. Wait a few seconds and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEbnUQOal0Uy"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "TENSORFLOW_UDF_CODE_PATH_IN_ARCHIVE=f\"data-science-examples-{GITHUB_BRANCH}/examples/tensorflow-with-gpu-preview/tensorflow_udf\"\n",
    "s=c.execute(f\"select list_files('{TENSORFLOW_UDF_CODE_BUCKETFS_PATH}/{TENSORFLOW_UDF_CODE_PATH_IN_ARCHIVE}')\")\n",
    "for t in s.fetchall():\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNLwkwgBRP7M"
   },
   "source": [
    "Now, we need the table definition of our dataset to derive our configuration for our Tensorflow Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6juFb2jXuiKX"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "c.execute(\"DESCRIBE fine_food_reviews;\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUKF5NlhRa4f"
   },
   "source": [
    "We need to define for each column which we want to use in our Model, what type of encoding the model should apply and the parameters for the encoding. The current implementation of the Tensorflow UDF provides following types of encoding:\n",
    "- float:\n",
    "  - Input and Output encoding using min-max scaling to get the values between 0 and 1. Currently, you need to provides min and max as parameters.\n",
    "  - The output encoding produces a mean squared error loss for the column\n",
    "  - Parameter:\n",
    "      - min_value\n",
    "      - max_value\n",
    "- categorical:\n",
    "  - Input:\n",
    "      - As input encoding of a categorical column the model will use an embedding layer with  hashing trick. \n",
    "  - Output:  \n",
    "    - As output encoding  of a categorical column the model will use an indicator column with hashing trick. \n",
    "    - The output encoding creates additionally a categorical cross entropy loss.\n",
    " - Parameters:\n",
    "      - hash_bucket_size\n",
    "      - embedding_dimensions\n",
    "  \n",
    "  NOTE: We use hashing in this example, because it does not required the generation of vocabularies. In practice, you need to test which encoding works best for your use case.\n",
    "- string:\n",
    "  - Strings currently only support input encodings and use for this Tensorflow Hub Modules, as such you should use this encoding only for Natural Language Text. \n",
    "    - Parameter: module_url\n",
    "  - For arbitrary character sequences, you need to add a new encoding and train it on your data from scratch\n",
    "  - If the sequence of characters only represents an ID than you can use categorical data. \n",
    "  - You could also train your own encoding for Natural Language Text, but this requires a huge amount of data and compute power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hChtrYlzdcbF"
   },
   "outputs": [],
   "source": [
    "save_model_name = \"test_model_4\"\n",
    "model_save_path_in_bucket=f\"udf/output/tensorflow/save/{save_model_name}\"\n",
    "MODEL_SAVE_BUCKETFS_URL=f\"http://w:write@localhost:6583/default/{model_save_path_in_bucket}\"\n",
    "MODEL_SAVE_BUCKETFS_PATH=f\"/buckets/bfsdefault/default/{model_save_path_in_bucket}\"\n",
    "load_model_name = \"test_model_4\"\n",
    "model_load_path_in_bucket=f\"udf/output/tensorflow/save/{load_model_name}\"\n",
    "MODEL_LOAD_BUCKETFS_URL=f\"http://w:write@localhost:6583/default/{model_load_path_in_bucket}\"\n",
    "MODEL_LOAD_BUCKETFS_PATH=f\"/buckets/bfsdefault/default/{model_load_path_in_bucket}\"\n",
    "config = f\"\"\"\n",
    "columns:\n",
    "  input:\n",
    "    PRODUCTID:\n",
    "      type: \"categorical\"\n",
    "      hash_bucket_size: 100000\n",
    "      embedding_dimensions: 100\n",
    "    USERID:\n",
    "      type: \"categorical\"\n",
    "      hash_bucket_size: 100000\n",
    "      embedding_dimensions: 100\n",
    "    SUMMARY:\n",
    "      type: string\n",
    "      module_url: \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "    TEXT:\n",
    "      type: string\n",
    "      module_url: \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "  output:\n",
    "    SCORE:\n",
    "      type: \"categorical\"\n",
    "      hash_bucket_size: 10\n",
    "      embedding_dimensions: 10 # gets ignored for outputs\n",
    "use_cache: false\n",
    "batch_size: 1000\n",
    "epochs: 5\n",
    "profile: false # TODO: currently not possible to profile, because the UDF Container misses the libcupti library\n",
    "device: \"/device:GPU:0\" # \"/cpu:0\"\n",
    "model_load_bucketfs_path: \"\" # \"{MODEL_LOAD_BUCKETFS_PATH}\" # Empty string means, do not try to load a model\n",
    "model_save_bucketfs_url: \"{MODEL_SAVE_BUCKETFS_URL}\"\n",
    "model_temporary_save_path: \"/tmp/save\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjgZ6D3BRcKJ"
   },
   "source": [
    "After you defined the config, you need to upload it into the BucketFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WW-r3EwGRcYJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "path_in_bucket = \"udf/config/tensorflow_config.yaml\"\n",
    "TENSORFLOW_CONFIG_BUCKETFS_PATH=f\"/buckets/bfsdefault/default/{path_in_bucket}\"\n",
    "TENSORFLOW_CONFIG_BUCKETFS_URL=f\"http://w:write@{CONNECTION_IP}:6583/default/{path_in_bucket}\"\n",
    "r=requests.put(TENSORFLOW_CONFIG_BUCKETFS_URL,data=config.lstrip().rstrip())\n",
    "print(f\"Put status code {r.status_code}\")\n",
    "r=requests.get(TENSORFLOW_CONFIG_BUCKETFS_URL)\n",
    "print(f\"Uploaded following config:\\n{r.content.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFJoMqzgRgLx"
   },
   "source": [
    "The UDF looks for a Connection \"tensorflow_config\" to get the BucketFS Path of the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZSVWA7qRgWT"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "sql=f\"\"\"\n",
    "CREATE OR REPLACE CONNECTION \"tensorflow_config\"\n",
    "TO 'file://{TENSORFLOW_CONFIG_BUCKETFS_PATH}';\n",
    "\"\"\"\n",
    "print(sql)\n",
    "c.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzzRQJI8uJri"
   },
   "source": [
    "Finally, you can now create the UDF script and start the training. The UDF script imports the Tensorflow UDF Code from the Bucketfs which previously downloaded from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNJkT0LanKka"
   },
   "outputs": [],
   "source": [
    "LIMIT=3000 # We use for faster demonstration only 3000 of the round about 600000\n",
    "\n",
    "c = connect(CONNECTION_IP)\n",
    "add_python3_script_language(c)\n",
    "set_script_output_address(c,INTERNAL_INSTANCE_IP)\n",
    "sql = textwrap.dedent(f\"\"\"\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT train_model(\n",
    "  \"PRODUCTID\" VARCHAR(10000),\n",
    "  \"USERID\" VARCHAR(10000),\n",
    "  \"SCORE\" INTEGER, \n",
    "  \"SUMMARY\" VARCHAR(2000000), \n",
    "  \"TEXT\" VARCHAR(2000000))\n",
    "EMITS(outputs VARCHAR(2000000)) AS \n",
    "def run(ctx):\n",
    "  import os\n",
    "  # Uncomment the following Line to use CPU only\n",
    "  #os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "  import sys\n",
    "  sys.path.append('{TENSORFLOW_UDF_CODE_BUCKETFS_PATH}/{TENSORFLOW_UDF_CODE_PATH_IN_ARCHIVE}')\n",
    "  from tensorflow_udf import TensorflowUDF\n",
    "  try:\n",
    "    TensorflowUDF().run(ctx,exa,train=True)\n",
    "  except Exception as e:\n",
    "    import logging\n",
    "    log = logging.getLogger(\"UDF\")\n",
    "    log.exception(\"Got exception\")\n",
    "    print(\"Abort\",flush=True)\n",
    "    raise e\n",
    "/\"\"\")\n",
    "c.execute(sql)\n",
    "try:\n",
    "  s=c.execute(f\"\"\"\n",
    "  select train_model(PRODUCTID,USERID,SCORE,SUMMARY,TEXT) \n",
    "  from (select * from fine_food_reviews limit {LIMIT}) q\n",
    "  \"\"\")\n",
    "  print(s.fetchall())\n",
    "  print(s.execution_time)\n",
    "except Exception as e:\n",
    "  print(\"Abort\",e)\n",
    "  c.abort_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXS_LXhKhJq0"
   },
   "outputs": [],
   "source": [
    "!bash fetch_output_redirect_from_last_statement.sh --zone \"$INSTANCE_ZONE\" \"$INSTANCE_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erwZvoLKmsua"
   },
   "source": [
    "Check if the UDF uploaded the checkpoints and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1h50iiCieWF"
   },
   "outputs": [],
   "source": [
    "c = connect(CONNECTION_IP)\n",
    "s=c.execute(f\"select list_files('{MODEL_SAVE_BUCKETFS_PATH}/checkpoints/tmp/save/checkpoints')\")\n",
    "for t in s.fetchall():\n",
    "  print(t)\n",
    "s=c.execute(f\"select list_files('{MODEL_SAVE_BUCKETFS_PATH}/metrics/tmp/save/metrics')\")\n",
    "for t in s.fetchall():\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T55e6bXIm4AI"
   },
   "source": [
    "Now that we are sure, that the metrics and checkpoints got uploaded to the bucketfs, we can download them to the notebook and visualize the metrics with tensorboard. Next, we install tensorflow v2 into the notebook, because it provides a jupyter extension to start tensorboard in jupyter or colab notebooks. Currently, tensorflow v2 is still in developement, as such we need to install the beta version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1lPF-GxU42A"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.0.0-beta1\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exKr2emUGPLX"
   },
   "outputs": [],
   "source": [
    "!rm -rf tmp\n",
    "!curl $MODEL_SAVE_BUCKETFS_URL/metrics.tar | tar -C . -xzf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn6p366RtrUy"
   },
   "source": [
    "To additionally download the checkpoints which contain information about the graph and the embeddings, you can use the following code, but be aware the checkpoints are quite large. A checkpoint is about 1.7 GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU7VZ6hUuIok"
   },
   "outputs": [],
   "source": [
    "!curl $MODEL_SAVE_BUCKETFS_URL/checkpoints.tar | tar -C . -xzf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeYcdncd-XR0"
   },
   "source": [
    "With the following command, you can start tensorboard within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkdZ2ieR41U0"
   },
   "outputs": [],
   "source": [
    "!kill $(pgrep tensorboard)\n",
    "%tensorboard --logdir tmp/save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UwIHqg9Kef4"
   },
   "source": [
    "## Teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mefQsRwQCGYB"
   },
   "source": [
    "In the end you should stop or delete the VM instances you created in the begining.\n",
    "\n",
    "NOTE: If you delete the instance you loose all data stored on this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtxGfyJkB7Yn"
   },
   "outputs": [],
   "source": [
    "!gcloud compute instances stop --zone $INSTANCE_ZONE $INSTANCE_NAME\n",
    "!tmux kill-session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Qc03xOPApkc"
   },
   "outputs": [],
   "source": [
    "!gcloud compute instances delete --zone $INSTANCE_ZONE $INSTANCE_NAME\n",
    "!tmux kill-session"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tensorflow-gpu-preview.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
