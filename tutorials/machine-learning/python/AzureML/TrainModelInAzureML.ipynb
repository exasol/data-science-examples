{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train an ML model on Exasol Data\n",
    "\n",
    "In this tutorial, you will load the data from Azure Blob Storage, and run a Python script as an AzureML job to preprocess the data and train a simple scikit-learn model. Then You will register the trained model with AzureML for further use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "You completed the [previous part of this tutorial series](ConnectAzureMLtoExasol.ipynb) and therefore have:\n",
    " - A running AzureML compute instance\n",
    " - An Azure Storage account\n",
    " - The [Scania Trucks](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) dataset loaded into Azure Blob Storage\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Python script for training the model\n",
    "\n",
    "We will use a Python script to create and train a SciKit-Learn model on the data we loaded from Exasol. You can find the script [here](main.py).\n",
    "The script loads the data from the files we saved in the Azure Blob Storage, does data preprocessing to combat the unbalanced nature of the dataset and removes empty values so the training can work properly.\n",
    "Then, it creates a simple SciKit-Learn model and trains it on the data. The model is evaluated using the test dataset and registered in the AzureML Workspace using MLflow.\n",
    "\n",
    "This script creates a model that only uses Python packages available in Exasol Saas UDFs natively. This means you can upload this model directly to your exasol Database and run it using an UDF. If your own models use different packages but you still need to run them on the cluster directly you need to [build and install yout own Script-Language Container](https://docs.exasol.com/db/latest/database_concepts/udf_scripts/adding_new_packages_script_languages.htm). Information on which packages are supported out of the box can be found [here](https://docs.exasol.com/saas/database_concepts/udf_scripts/python3.htm).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare AzureML studio to run the Python script\n",
    "\n",
    "This notebook is meant to be run in AzureML Studio, so upload it to your Notebooks, open it and select your compute instance in the drop-down menu at the top of your notebook. The same steps can be achieved by accessing AzureML using remote scripts, but for demonstration purposes we use AzureML Studio here.\n",
    "\n",
    "First, we install some AzureML functionality."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install azure-identity\n",
    "!pip install azure-ai-ml==1.3.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we create an MLClient for accessing our AzureML jobs programmatically. For this we need our AzureML subscription id, resource group name and workspace name. If you are not sure what your resource group name is, you can find it by clicking your subscription in the top left oft AzureML Studio\n",
    "Make sure to use the workspace you set up in the previous tutorial.\n",
    "\n",
    "![](img_src/resource_group.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"<your subscription id>\",               # change\n",
    "    resource_group_name=\"<your resource group name>\",       # change\n",
    "    workspace_name=\"<your workspace name>\",                 # change\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a new Python Environment\n",
    "\n",
    "To run our Python script we need to create a new environment and install the required dependencies. For this, we first create a new directory called \"dependencies\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make env\n",
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order for our model to be usable in the Exasol Saas Database later, we need to make sure the SciKit-learn version we use matches the version in Saas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - scikit-learn=1.0.2\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will create a new environment to run our job in. We will use the new dependencies file and use an Ubuntu image as the base for our environment. Then we will create the new environment on our *MLClient*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mentities\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Environment\n\u001B[1;32m      2\u001B[0m custom_env_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<Name your environment here>\u001B[39m\u001B[38;5;124m\"\u001B[39m    \u001B[38;5;66;03m# change\u001B[39;00m\n\u001B[1;32m      4\u001B[0m pipeline_job_env \u001B[38;5;241m=\u001B[39m Environment(\n\u001B[1;32m      5\u001B[0m     name\u001B[38;5;241m=\u001B[39mcustom_env_name,\n\u001B[1;32m      6\u001B[0m     description\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustom environment for azureML tut\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     image\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m )\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "custom_env_name = \"<Name your environment here>\"    # change\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for AzureML tutorial\",\n",
    "    tags={\"scikit-learn\": \"1.0.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Python script\n",
    "\n",
    "Now we need to create an AzureML job with the following inputs:\n",
    "\n",
    " - The path to the Python script\n",
    " - A command to run the script\n",
    " - Information which AzureML Compute and Environment to use\n",
    "\n",
    "This job will be used to run our Python script on our Compute using the environment we created in the step before.\n",
    "The script takes links to the data files we loaded ino Azure Blob Storage in the previous tutorial as input. You can find these links by naviating to your data files in your data store and clicking the kebab menu besides each file. A drop down menu will open where you can select the \"Copy URI\" option. This opens a pop-up window where you can copy the link to the file.\n",
    "![](img_src/get_data_link.png)\n",
    "\n",
    "This opens a pop-up window where you can copy the link to the file.\n",
    "![](img_src/get_data_link_2.png)\n",
    "\n",
    "Also don't forget to change the variables for your Compute.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        train_data=Input(\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            path=\"< link to training data file >\",       # change\n",
    "        ),\n",
    "        test_data=Input(\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            path=\"< link to test data file >\",       # change\n",
    "        ),\n",
    "        learning_rate=0.05\n",
    "    ),\n",
    "    code=\".\",  # location of source code\n",
    "    command=\"python main.py --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} --learning_rate ${{inputs.learning_rate}}\",\n",
    "    environment=pipeline_job_env,\n",
    "    compute=\"<your_compute_name>\",                      # change\n",
    "    experiment_name=\"<experiment_name>\",                # change\n",
    "    display_name=\"<experiment_name_>\",                  # change\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can run the script on our compute instance. A link will show up below, which you can click on to see the job details and output logs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.create_or_update(job)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Confusion Matrix of our trained model.\n",
    "\n",
    "\n",
    "|            | predicted neg  | predicted pos  |\n",
    "|------------|----------------|----------------|\n",
    "|actual neg  |        14841   | 784            |\n",
    "|actual pos  |           13   | 362            |\n",
    "\n",
    "The model has a total cost of 14340 according to the ida-score we implemented in accordance to the problem description of the Scania Trucks dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the trained model\n",
    "\n",
    "The script will directly register the trained model in your AzureML Workspace, so you can use it to run inference in AzureML. It will also save the model in the output of the job. From there, you can extract it to run it in your Exasol cluster. You can find your registered model under the Assets, Model entry in the AzureML Studio menu on the left.\n",
    "\n",
    "![](img_src/registered_model.png)\n",
    "\n",
    "Now that we have trained and registered a model on the data we imported from our Exasol Saas instance, we can move on to the\n",
    "[next part](InvokeModelFromExasolDBwithUDF.ipynb), where we will use this model from with in our Exasol Cluster to classify some data."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}