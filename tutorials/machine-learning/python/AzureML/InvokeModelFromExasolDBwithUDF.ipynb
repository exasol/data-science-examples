{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deploy the trained Model and invoke it using Exasol\n",
    "\n",
    "In this part of the tutorial we will show how to deploy and invoke the trained model using Exasol. For this we will show two versions.\n",
    "You can either:\n",
    "\n",
    "\n",
    "   * [Deploy the model using an AzureML online Endpoint and then invoke it via an Exasol UDF](#deploy-the-model-in-an-azureml-endpoint)\n",
    "\n",
    "Or:\n",
    "   * [Load the model into Exasols Filesystem (BucketFS), and then deploy and invoke it via an Exasol UDF](#deplay-and-invoke-the-model-in-exasol)\n",
    "\n",
    "Which version you choose is up to you.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy the model in an AzureML endpoint\n",
    "\n",
    "In this Section we will explain how to Deploy the model in an AzureML endpoint, and the invoke it via an Exasol UDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    " ## TODO\n",
    " * AzureML\n",
    " * Compute\n",
    " * trained and registered model\n",
    " * exasol with dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create online endpoint in azureml\n",
    "\n",
    "First we need to set up an online endpoint with our trained and registered model in AzureML, so we can use it for real time inferencing. We will do this using Python in an AzureML Notebook. You can find the AzureML tutorial for this [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load this notebook into your AzureML Notebooks and start your Compute. Then we can install some dependencies if not already installed from previous steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install azure-identity\n",
    "!pip install azure-ai-ml==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the required libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [319]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLClient\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mai\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mentities\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      3\u001B[0m     ManagedOnlineEndpoint,\n\u001B[1;32m      4\u001B[0m     ManagedOnlineDeployment,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     CodeConfiguration,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mazure\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01midentity\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DefaultAzureCredential\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can enter our Credentials to access our workspace."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"<your subscription id>\",               # change\n",
    "    resource_group_name=\"<your resource group name>\",       # change\n",
    "    workspace_name=\"<your workspace name>\",                 # change\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are set up to start to set up our online endpoint we then want to deploy our model in. We set it up with key authentication, but you could use token authentication instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define an endpoint name\n",
    "endpoint_name = \"<your-endpoint-name>\"                      # change\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name = endpoint_name,\n",
    "    description=\"<some description>\",                       # change\n",
    "    auth_mode=\"key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the Endpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make a Deployment for the model\n",
    "\n",
    "We now need to Deploy our model to the Endpoint via an AzureML Deployment. For this we need an Environment definition which can run our model. This need to include the \"azureml-inference-server-http\" package. There are also ready made images available from Microsoft, but here we make our own.\n",
    "We take the conda file that is saved in our registered MLflow model and edit it to include the \"azureml-inference-server-http\" package. Then write it to a file.\n",
    "#### (TODO explain how find?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ./conda.yml\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8.16\n",
    "- pip<=23.1.2\n",
    "- pip:\n",
    "  - azureml-inference-server-http\n",
    "  - mlflow==1.26.1\n",
    "  - cloudpickle==2.2.1\n",
    "  - scikit-learn==1.0.2\n",
    "name: endpoint-env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get a handle to our registered model and create the Environment for our Deployment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "model = ml_client.models.get(name=\"<your registered model name>\", version=\"<version of your model you want to use>\")  # change\n",
    "\n",
    "env = Environment(\n",
    "    name=\"<name the Environment>\",                                      # change\n",
    "    description=\"Custom environment for azureML tutorial endpoint\",\n",
    "    conda_file=\"./conda.yml\",                                           # change if necessary, path to your conda.yaml fil we created earlier\n",
    "    image=\"mcr.microsoft.com/azureml/minimal-ubuntu20.04-py38-cpu-inference:latest\", # base image from microsoft we use\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this we now can create our Deployment.\n",
    "The Deployment uses a [scoring script](score.py). This script has an \"init()\" function which loads the model, and a \"run\" function which takes the input data and feeds it to the model. This function returns the classification results. Make sure you have the scoring script in your AzureML files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CodeConfiguration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [320]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cc \u001B[38;5;241m=\u001B[39m \u001B[43mCodeConfiguration\u001B[49m(code\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, scoring_script\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore.py\u001B[39m\u001B[38;5;124m\"\u001B[39m)     \u001B[38;5;66;03m# change if necessary to point to your scoring script in AzureML\u001B[39;00m\n\u001B[1;32m      2\u001B[0m model_deployment \u001B[38;5;241m=\u001B[39m ManagedOnlineDeployment(\n\u001B[1;32m      3\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<name your deployment>\u001B[39m\u001B[38;5;124m\"\u001B[39m,          \u001B[38;5;66;03m# change\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     endpoint_name\u001B[38;5;241m=\u001B[39mendpoint_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     instance_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     10\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'CodeConfiguration' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "cc = CodeConfiguration(code=\".\", scoring_script=\"score.py\")     # change if necessary to point to your scoring script in AzureML\n",
    "model_deployment = ManagedOnlineDeployment(\n",
    "    name=\"<name your deployment>\",          # change\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=cc,\n",
    "    instance_type=\"Standard_DS1_v2\",        # Type of Azure Instance. You can change this if you need more computing power\n",
    "    instance_count=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create the Deployment on our endpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(model_deployment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check the status of your Deployment by geting the logs with this comand. Alternativly you can also Navigate to your Deployment in AzureML in your Browser and chack status and logs there.\n",
    "### todo screenshots?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"<your deployment name>\", endpoint_name=endpoint_name, lines=50\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to access this Endpoint we will need a key (or a token if you choose to go with token authentication above). Get the key like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endpoint_key = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "print(endpoint_key.primary_key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Invoke the Endpoint from Exasol\n",
    "\n",
    "In order to invoke our deployed Endpoint, we first need access to our Exasol Saas Database. We will use pyExasol for this.\n",
    "### todo link"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyexasol in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (0.24.0)\r\n",
      "Requirement already satisfied: pyopenssl in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from pyexasol) (22.0.0)\r\n",
      "Requirement already satisfied: websocket-client>=1.0.1 in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from pyexasol) (1.2.3)\r\n",
      "Requirement already satisfied: rsa in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from pyexasol) (4.8)\r\n",
      "Requirement already satisfied: cryptography>=35.0 in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from pyopenssl->pyexasol) (36.0.1)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from cryptography>=35.0->pyopenssl->pyexasol) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl->pyexasol) (2.21)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/marlene/PycharmProjects/data-science-examples/venv/lib/python3.8/site-packages (from rsa->pyexasol) (0.4.8)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 23.2.1 is available.\r\n",
      "You should consider upgrading via the '/home/marlene/PycharmProjects/data-science-examples/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyexasol\n",
    "import pyexasol\n",
    "EXASOL_HOST = \"<your>.clusters.exasol.com\"      # change\n",
    "EXASOL_PORT = \"8563\"                            # change if needed\n",
    "EXASOL_USER = \"<your-exasol-user>\"              # change\n",
    "EXASOL_PASSWORD = \"exa_pat_<your_password>\"     # change\n",
    "EXASOL_SCHEMA = \"IDA\"                           # change if needed\n",
    "\n",
    "EXASOL_CONNECTION = f\"{EXASOL_HOST}:{EXASOL_PORT}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "exasol = pyexasol.connect(dsn=EXASOL_CONNECTION, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now use a Python UDF(link?) to call the AzureML Online Endpoint we just created and use it to classify out test data we loaded into Exasol Saas in the first part of this tutorial(link).\n",
    "Take care to only use the supported(link) packages, or build your own language container(link). The udf takes the data in the provided table and sends is as a REST request to the Endpoint. The UDF will then output the returned result.\n",
    "\n",
    "### TODO links\n",
    "explain where find deployment url\n",
    "\n",
    "Change the endpoint key (will be different for each time you create the endpoint), the scoring url and the deployment name to access your endpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "<ExaStatement session_id=1778269931084447746 stmt_idx=2>"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_open_schema = \"\"\"OPEN SCHEMA \"IDA\";\"\"\"\n",
    "\n",
    "sql_create_invoke_endpoint =\"\"\"\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT IDA.invoke_endpoint(...)\n",
    "EMITS (\"ID\" DECIMAL(20,0), \"result\" VARCHAR(200)) AS\n",
    "\n",
    "import requests\n",
    "import ujson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def run(ctx):\n",
    "    # set up needed info to send the request\n",
    "    endpoint_key = \"<your endpoint key>\"                                # change\n",
    "    deployment_name = \"<your deployment name>\"                          # change\n",
    "    headers = {'Content-Type':'application/json',\n",
    "                'Authorization': f'Bearer {endpoint_key}',\n",
    "               'azureml-model-deployment': f'{deployment_name}'}\n",
    "    scoring_url = \"<scoring url>\"                                       # change\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # get the data\n",
    "        try:\n",
    "            df = ctx.get_dataframe(500)\n",
    "        except:\n",
    "            return 0\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        # remove the label from the test data\n",
    "        id_column = df[\"0\"]\n",
    "        df = df.drop(\"0\", 1)\n",
    "\n",
    "        # cast data to floats and switch \"nan\" to \"null\" so it will encode and decode as a valid json\n",
    "        df = df.apply(pd.to_numeric, downcast='float', errors='ignore')\n",
    "        df_str = str((df.values).tolist())\n",
    "        df_str = df_str.replace(\"nan\", 'null')\n",
    "\n",
    "        PARAMS = '{\"data\": ' + df_str + '}'\n",
    "\n",
    "        # send the request\n",
    "        try:\n",
    "            result = requests.post(url=scoring_url, data=PARAMS, headers=headers)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "        str_l = re.findall('[0,1]', result.text)\n",
    "        df_res = pd.DataFrame(str_l)\n",
    "        ctx.emit(pd.concat([id_column, df_res], axis=1))\n",
    "/\n",
    "\"\"\"\n",
    "\n",
    "exasol.execute(sql_open_schema)\n",
    "exasol.execute(sql_create_invoke_endpoint)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to select the columns we want to send to the Endpoint. These should be the same we used to train the endpoint in the part of the tutorial(link).\n",
    "### TODO link"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "column_names = ['AA_000', 'AG_005', 'AH_000', 'AL_000', 'AM_0', 'AN_000', 'AO_000', 'AP_000', 'AQ_000',\n",
    "                    'AZ_004', 'BA_002', 'BB_000', 'BC_000', 'BD_000', 'BE_000',\n",
    "                    'BF_000', 'BG_000', 'BH_000', 'BI_000', 'BJ_000', 'BS_000', 'BT_000', 'BU_000', 'BV_000',\n",
    "                    'BX_000', 'BY_000', 'BZ_000', 'CA_000', 'CB_000', 'CC_000', 'CI_000', 'CN_004', 'CQ_000',\n",
    "                    'CS_001', 'DD_000', 'DE_000', 'DN_000', 'DS_000', 'DU_000', 'DV_000', 'EB_000', 'EE_005']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That is all we need to call our UDF which will call the Endpoint from within Exasol and the return te classification results to us. The call also adds a ROWID to be able to sort the result to the right input, so we can then check how many where correctly classified by our endpoint.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "res = exasol.export_to_pandas(\"\"\"SELECT \"CLASS\", \"result\" FROM  (\n",
    "                           SELECT IDA.invoke_endpoint(ROWID, {columns!q}) FROM IDA.TEST t) r\n",
    "                           JOIN IDA.TEST o ON r.ID = o.ROWID\"\"\", {\"columns\": column_names})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can create a confusion matrix from our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "predictions      0    1\nactuals                \nneg          14841  784\npos             13  362",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>predictions</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>actuals</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neg</th>\n      <td>14841</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>pos</th>\n      <td>13</td>\n      <td>362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(index=res['CLASS'], columns=res[\"result\"], rownames=['actuals'], colnames=['predictions'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don’t forget to Delete the endpoint once you don’t need it anymore, and also close your pyexasol connection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exasol.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and invoke the model in Exasol\n",
    "\n",
    "Let's look into how to deploy and invoke the model directly on the Exasol Saas Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "\n",
    "* trained model\n",
    "* Exasol Saas with data\n",
    "* PyExasol(link)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to download the model from AzureML and upload it into an Exasol Bucket in the BucketFS (Exasol's built-in Filesystem).\n",
    "## todo explain how download\n",
    "Then we can upload it to the BucketFS using the online Saas interface.\n",
    "# pics and explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### todo Check if uploaded, get path from Saas, pics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we will use PyExasol to connect to our Exasol Saas Cluster."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyexasol\n",
    "import pyexasol\n",
    "import pandas as pd\n",
    "\n",
    "EXASOL_HOST = \"<your>.clusters.exasol.com\"      # change\n",
    "EXASOL_PORT = \"8563\"                            # change if needed\n",
    "EXASOL_USER = \"<your-exasol-user>\"              # change\n",
    "EXASOL_PASSWORD = \"exa_pat_<your_password>\"     # change\n",
    "EXASOL_SCHEMA = \"IDA\"                           # change if needed\n",
    "\n",
    "# get the connection\n",
    "EXASOL_CONNECTION = f\"{EXASOL_HOST}:{EXASOL_PORT}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make sure your ip is whitelisted in exasol saas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "exasol = pyexasol.connect(dsn=EXASOL_CONNECTION, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, move model from AzureML to Exasol BucketFS.\n",
    "open saas, start db, clck manage udf files, click new de, upload file (only works in enterprise edition)\n",
    "copy path of file\n",
    "\n",
    "## call and invoke using udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExaStatement session_id=1774377255978401792 stmt_idx=1>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_open_schema = \"\"\"OPEN SCHEMA \"IDA\";\"\"\"\n",
    "sql_ls = \"\"\" --/\n",
    "CREATE OR REPLACE PYTHON3 SCALAR SCRIPT \"IDA.LS\" (\"my_path\" VARCHAR(100)) EMITS (\"FILES\" VARCHAR(100)) AS\n",
    "import os\n",
    "def run(ctx):\n",
    "    for line in os.listdir(ctx.my_path):\n",
    "        ctx.emit(line)\n",
    "/\n",
    "\"\"\"\n",
    "\n",
    "sgl_run_ls = \"\"\"SELECT \"IDA.LS\"('/buckets/uploads/default/testfolder');\"\"\"\n",
    "\n",
    "exasol.execute(sql_open_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExaStatement session_id=1774293578104963073 stmt_idx=16>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exasol.execute(sql_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FILES\n",
       "0  model.pkl"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exasol.export_to_pandas(sgl_run_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now use a Python UDF(link?) to load the model and use it to classify out test data we loaded into Exasol Saas in the first part of this tutorial(link). Take care to only use the supported(link) packages, or build your own language container(link). The udf takes the data in the provided table and uses the model to classify it. The UDF will then output the result.\n",
    "\n",
    "## todo chang to set script?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<ExaStatement session_id=1778190569628434435 stmt_idx=33>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_create_inference_udf = \"\"\"\n",
    "--/\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT IDA.use_model_for_inference(...)\n",
    "EMITS (\"ID\" DECIMAL(20,0), \"prediction\" DOUBLE) AS\n",
    "\n",
    "import urllib.request\n",
    "import lxml.etree as etree\n",
    "import sklearn\n",
    "import numpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_model():\n",
    "    model_path = \"/buckets/uploads/default/testfolder/model.pkl\"  # change to your model file path\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = pickle.load(open(model_path, 'rb'))\n",
    "    return model\n",
    "\n",
    "def infer(data, model):\n",
    "\n",
    "    data = numpy.array(data)\n",
    "    result = model.predict(data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def run(ctx):\n",
    "    model = load_model()\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(num_rows=1000)\n",
    "        if df is None:\n",
    "            break\n",
    "        id_column = df[\"0\"]\n",
    "        df = df.drop(\"0\", 1)\n",
    "        response = infer(df, model)\n",
    "        result = pd.DataFrame(response)\n",
    "        ctx.emit(pd.concat([id_column,result],axis=1))\n",
    "\n",
    "/\n",
    "\"\"\"\n",
    "exasol.execute(sql_create_inference_udf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to select the columns we want to send to the Endpoint. These should be the same we used to train the endpoint in the part of the tutorial(link).\n",
    "### TODO link"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['AA_000', 'AG_005', 'AH_000', 'AL_000', 'AM_0', 'AN_000', 'AO_000', 'AP_000', 'AQ_000',\n",
    "                    'AZ_004', 'BA_002', 'BB_000', 'BC_000', 'BD_000', 'BE_000',\n",
    "                    'BF_000', 'BG_000', 'BH_000', 'BI_000', 'BJ_000', 'BS_000', 'BT_000', 'BU_000', 'BV_000',\n",
    "                    'BX_000', 'BY_000', 'BZ_000', 'CA_000', 'CB_000', 'CC_000', 'CI_000', 'CN_004', 'CQ_000',\n",
    "                    'CS_001', 'DD_000', 'DE_000', 'DN_000', 'DS_000', 'DU_000', 'DV_000', 'EB_000', 'EE_005']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now call the UDF to predict on our test data using the model we trained in Exasol. The call also adds a ROWID to be able to sort the result to the right input, so we can then check how many where correctly classified by our endpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = exasol.export_to_pandas(\"\"\"SELECT \"CLASS\", \"prediction\" FROM  (\n",
    "                           SELECT IDA.use_model_for_inference(ROWID, {columns_without_class!q}) FROM IDA.TEST t) r\n",
    "                           JOIN IDA.TEST o ON r.ID = o.ROWID\"\"\", {\"columns_without_class\": column_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can create a confusion matrix from our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "predictions      0    1\nactuals                \nneg          14841  784\npos             13  362",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>predictions</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>actuals</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neg</th>\n      <td>14841</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>pos</th>\n      <td>13</td>\n      <td>362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(index=res['CLASS'], columns=res[\"prediction\"], rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't forget to close your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exasol.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}