{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deploy the trained Model and invoke it using Exasol\n",
    "\n",
    "In this part of the tutorial we will show how to deploy and invoke the trained model using Exasol. For this we will show two versions.\n",
    "You can either:\n",
    "\n",
    "\n",
    "   * [Deploy the model using an AzureML online Endpoint and then invoke it via an Exasol UDF](#deploy-the-model-in-an-azureml-endpoint)\n",
    "\n",
    "Or:\n",
    "   * [Load the model into Exasols Filesystem (BucketFS), and then deploy and invoke it via an Exasol UDF](#deplay-and-invoke-the-model-in-exasol)\n",
    "\n",
    "Which version you choose is up to you.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy the model in an AzureML endpoint\n",
    "\n",
    "In this Section we will explain how to Deploy the model in an AzureML endpoint, and then invoke it via an Exasol UDF."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "\n",
    "You completed the [previous part of this tutorial series](TrainModelInAzureML.ipynb) and therefore have:\n",
    " * A running AzureML compute instance\n",
    " * A SciKit-learn MLflow model trained on the data registered in AzureML\n",
    " * The [Scania Trucks](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) dataset loaded into an Exasol Saas Database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Online Endpoint in AzureML\n",
    "\n",
    "First we need to set up an Online Endpoint with our trained and registered model in AzureML, so we can use it for real time inferencing. We will do this using Python in an AzureML Notebook. You can find the AzureML tutorial for this [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=python).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load this notebook into your AzureML Notebooks and start your Compute. Then we can install some dependencies if not already installed from previous steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install azure-identity\n",
    "!pip install azure-ai-ml==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the required Python3.8 libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we can enter the Credentials to access our workspace."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"<your subscription id>\",               # change\n",
    "    resource_group_name=\"<your resource group name>\",       # change\n",
    "    workspace_name=\"<your workspace name>\",                 # change\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to start to set up our Online Endpoint we want to deploy our model in. We use key authentication for the Online Endpoint, but you could use token authentication instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define an endpoint name\n",
    "endpoint_name = \"<your-endpoint-name>\"                      # change\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name = endpoint_name,\n",
    "    description=\"<some description>\",                       # change\n",
    "    auth_mode=\"key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the Endpoint. This can take a few minutes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make a Deployment for the model\n",
    "\n",
    "We now need to Deploy our model to the Endpoint via an AzureML Deployment. For this we need an Environment definition which can run our model. This needs to include the \"azureml-inference-server-http\" package. There are also ready-made images available from Microsoft, but here we make our own.\n",
    "We take the conda file that is saved in our registered MLflow model and edit it to include the \"azureml-inference-server-http\" package. Then write it to a file.\n",
    "\n",
    "You can find your registered Model and its artifacts in AzureML Studio. Select \"Models\" in the menu on the left and click on the model you want to use. From there select the \"Artifacts\" tab to find the model files which include the conda.yaml.\n",
    "\n",
    "![](img_src/registered_model.png)\n",
    "\n",
    "![](img_src/conda_file_artifact.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile ./conda.yml\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.8.16\n",
    "- pip<=23.1.2\n",
    "- pip:\n",
    "  - azureml-inference-server-http\n",
    "  - mlflow==1.26.1\n",
    "  - cloudpickle==2.2.1\n",
    "  - scikit-learn==1.0.2\n",
    "name: endpoint-env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get a handle to our registered model and create the Environment for our Deployment using our new conda file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "model = ml_client.models.get(name=\"<your registered model name>\", version=\"<version of your model you want to use>\")  # change\n",
    "\n",
    "env = Environment(\n",
    "    name=\"<name the Environment>\",                                      # change\n",
    "    description=\"Custom environment for azureML tutorial endpoint\",\n",
    "    conda_file=\"./conda.yml\",                                           # change if necessary, path to your conda.yaml file we created earlier\n",
    "    image=\"mcr.microsoft.com/azureml/minimal-ubuntu20.04-py38-cpu-inference:latest\", # base image from microsoft we use\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this we can create our Deployment.\n",
    "The Deployment uses a [scoring script](score.py). This script has an \"init()\" function which loads the model, and a \"run\" function which takes the input data and feeds it to the model. This function returns the classification results. Make sure you have the scoring script in your AzureML files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "\n",
    "cc = CodeConfiguration(code=\".\", scoring_script=\"score.py\")     # change if necessary to point to your scoring script in AzureML\n",
    "model_deployment = ManagedOnlineDeployment(\n",
    "    name=\"<name your deployment>\",          # change\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=cc,\n",
    "    instance_type=\"Standard_DS1_v2\",        # Type of Azure Instance. You can change this if you need more computing power\n",
    "    instance_count=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create the Deployment on our endpoint. This can also take some minutes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(model_deployment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check the status of your Deployment by getting the logs with this command. Azure Studio also sends you a Notification when the Deployment is done. If you want a more detailed look you can also Navigate to your Deployment in AzureML and check status and logs there. To do this go to the menu on the left, select \"Endpoints\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"<your deployment name>\", endpoint_name=endpoint_name, lines=50\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to call this Endpoint we will need a key (or a token if you choose to go with token authentication). Here is how to access the key. You will need it in the UDF below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endpoint_key = ml_client.online_endpoints.get_keys(name=endpoint_name)\n",
    "print(endpoint_key.primary_key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Invoke the Endpoint from Exasol\n",
    "\n",
    "In order to invoke our deployed Endpoint, we first need access to our Exasol Saas Database. We will use the [PyExasol](https://docs.exasol.com/db/latest/connect_exasol/drivers/python/pyexasol.htm) package for this. Install and import it. Then enter your connection info. It can happen that the connection info you used previously is not valid anymore. In that case you can find a more detailed explanation on how to allow a connection in Exasol Saas in [the Introduction to this tutorial series](Introduction.ipynb). Make sure the ip you are attempting to connect from is whitelisted in Exasol Saas and the authentication token you use as a password is still valid. Generate a new one if necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyexasol\n",
    "import pyexasol"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "EXASOL_HOST = \"<your>.clusters.exasol.com\"      # change\n",
    "EXASOL_PORT = \"8563\"                            # change if needed\n",
    "EXASOL_USER = \"<your-exasol-user>\"              # change\n",
    "EXASOL_PASSWORD = \"exa_pat_<your_password>\"     # change\n",
    "EXASOL_SCHEMA = \"IDA\"                           # change if needed\n",
    "\n",
    "EXASOL_CONNECTION = f\"{EXASOL_HOST}:{EXASOL_PORT}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "exasol = pyexasol.connect(dsn=EXASOL_CONNECTION, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now use a Python UserDefinedFunction (UDF) to call the AzureML Online Endpoint we just created and use it to classify the test data we loaded into Exasol Saas in the [first part of this tutorial series](ConnectAzureMLtoExasol.ipynb). You can find the documentation for Exasol UDFs [here](https://docs.exasol.com/saas/database_concepts/udf_scripts.htm).\n",
    "Take care to only use the [supported](https://docs.exasol.com/saas/database_concepts/udf_scripts/python3.htm) packages, or build your own [Script-Language-Container](https://github.com/exasol/script-languages-container-tool). The UDF takes the data in the provided table and sends it as a REST request to the Endpoint. The UDF will then output the returned result.\n",
    "\n",
    "You will need to change the information for accessing you AzureML Online Endpoint. Set the name of the Deployment we created previously, and the Endpont key we got earlier. This key will change each time you recreate the Endpoint, so take care to update it.\n",
    "\n",
    "You will also need the URL for your deployment. You can find it by Navigating to your Endpoint in AzureML Studio and selecting the \"Consume\" tab. Here you can also find code snippets that show you how you can access the Endpoint.\n",
    "\n",
    "![](img_src/consume_endpoint.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below yu can see the UDF we use to access our endpoint. First we need to create the UDF in the Exasol Database using our PyExasol connection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "sql_open_schema = \"\"\"OPEN SCHEMA \"IDA\";\"\"\"\n",
    "\n",
    "sql_create_invoke_endpoint =\"\"\"\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT IDA.invoke_endpoint(...)\n",
    "EMITS (\"ID\" DECIMAL(20,0), \"result\" VARCHAR(200)) AS\n",
    "\n",
    "import requests\n",
    "import ujson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "def run(ctx):\n",
    "    # set up needed info to send the request\n",
    "    endpoint_key = \"<your endpoint key>\"                                # change\n",
    "    deployment_name = \"<your deployment name>\"                          # change\n",
    "    headers = {'Content-Type':'application/json',\n",
    "                'Authorization': f'Bearer {endpoint_key}',\n",
    "               'azureml-model-deployment': f'{deployment_name}'}\n",
    "    scoring_url = \"<scoring url>\"                                       # change\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # get the data\n",
    "        try:\n",
    "            df = ctx.get_dataframe(500)\n",
    "        except:\n",
    "            return 0\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        # remove the label from the test data\n",
    "        id_column = df[\"0\"]\n",
    "        df = df.drop(\"0\", 1)\n",
    "\n",
    "        # cast data to floats and switch \"nan\" to \"null\" so it will encode and decode as a valid json\n",
    "        df = df.apply(pd.to_numeric, downcast='float', errors='ignore')\n",
    "        df_str = str((df.values).tolist())\n",
    "        df_str = df_str.replace(\"nan\", 'null')\n",
    "\n",
    "        PARAMS = '{\"data\": ' + df_str + '}'\n",
    "\n",
    "        # send the request\n",
    "        try:\n",
    "            result = requests.post(url=scoring_url, data=PARAMS, headers=headers)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "        str_l = re.findall('[0,1]', result.text)\n",
    "        df_res = pd.DataFrame(str_l)\n",
    "        ctx.emit(pd.concat([id_column, df_res], axis=1))\n",
    "/\n",
    "\"\"\"\n",
    "\n",
    "exasol.execute(sql_open_schema)\n",
    "exasol.execute(sql_create_invoke_endpoint)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to select the columns we want to send to the Endpoint. These should be the same we used to train the endpoint in the [training part of the tutorial](TrainModelInAzureML.ipynb)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "column_names = ['AA_000', 'AG_005', 'AH_000', 'AL_000', 'AM_0', 'AN_000', 'AO_000', 'AP_000', 'AQ_000',\n",
    "                    'AZ_004', 'BA_002', 'BB_000', 'BC_000', 'BD_000', 'BE_000',\n",
    "                    'BF_000', 'BG_000', 'BH_000', 'BI_000', 'BJ_000', 'BS_000', 'BT_000', 'BU_000', 'BV_000',\n",
    "                    'BX_000', 'BY_000', 'BZ_000', 'CA_000', 'CB_000', 'CC_000', 'CI_000', 'CN_004', 'CQ_000',\n",
    "                    'CS_001', 'DD_000', 'DE_000', 'DN_000', 'DS_000', 'DU_000', 'DV_000', 'EB_000', 'EE_005']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That is all we need to call our UDF. It will call the Endpoint from within Exasol and then return the classification results to us. The call also adds a ROWID to be able to sort the result to the right input. This way we can then check how many where correctly classified by our endpoint.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "res = exasol.export_to_pandas(\"\"\"SELECT \"CLASS\", \"result\" FROM  (\n",
    "                           SELECT IDA.invoke_endpoint(ROWID, {columns!q}) FROM IDA.TEST t) r\n",
    "                           JOIN IDA.TEST o ON r.ID = o.ROWID\"\"\", {\"columns\": column_names})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can create a confusion matrix from our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "predictions      0    1\nactuals                \nneg          14841  784\npos             13  362",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>predictions</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>actuals</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neg</th>\n      <td>14841</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>pos</th>\n      <td>13</td>\n      <td>362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(index=res['CLASS'], columns=res[\"result\"], rownames=['actuals'], colnames=['predictions'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Don’t forget to Delete the endpoint once you don’t need it anymore, and also close your pyexasol connection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exasol.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and invoke the model in Exasol\n",
    "\n",
    "Let's look into how to deploy and invoke the model directly on the Exasol Saas Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "\n",
    "You completed the [previous part of this tutorial series](TrainModelInAzureML.ipynb) and therefore have:\n",
    " * A SciKit-learn MLflow model trained on the data registered in AzureML\n",
    " * Exasol Database Enterprise Edition(Needed for file System Access)\n",
    " * The [Scania Trucks](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) dataset loaded into an Exasol Saas Database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to download the model from AzureML and upload it into an Exasol Bucket in the BucketFS (Exasol's built-in Filesystem).\n",
    "For this, navigate to your registered model by clicking on the \"Models\" entry in the menu on the left in AzureML Studio. Select the model you want to use. From here you can either download all model files by clicking \"Download all\", or you can go to the \"Artifacts\" tab and only download the \"model.pkl\" file we will need for this tutorial. The use of the MLflow package is not currently supported by Exasol, so we do not need the other files created by MLflow.\n",
    "![](img_src/download_all.png)\n",
    "![](img_src/download_file_arifact.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you downloaded the model, go to the online interface of your Exasol Saas Database. Click on the three dots on th right and then click \"Manage UDF files\". This step only works in the Enterprise edition of Exasol Saas.\n",
    "![](img_src/manage_udf_files.png)\n",
    "\n",
    "This leads you to an interface where you can upload files to the internal file system of Exasol Saas, also called the BucketFS. Click on the \"Upload files\" button and upload your model.pkl file. It will then show you the path where your fle is saved at. Remember that path, as we will need it to access the file from within the UDF. (It might be that the path does not work. in that case try replacing the \"bucketfs\" in the front of the path with \"buckets\" instead)\n",
    "![](img_src/file_path_bucketfs.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we will use the [PyExasol](https://docs.exasol.com/db/latest/connect_exasol/drivers/python/pyexasol.htm) package to connect to our Exasol Saas Cluster. Install and import it. Then enter your connection info. It can happen that the connection info you used previously is not valid anymore. In that case you can find a more detailed explanation on how to allow a connection in Exasol Saas in [the Introduction to this tutorial series](Introduction.ipynb). Make sure the ip you are attempting to connect from is whitelisted in Exasol Saas and the authentication token you use as a password is still valid. Generate a new one if necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyexasol\n",
    "import pyexasol\n",
    "import pandas as pd\n",
    "\n",
    "EXASOL_HOST = \"<your>.clusters.exasol.com\"      # change\n",
    "EXASOL_PORT = \"8563\"                            # change if needed\n",
    "EXASOL_USER = \"<your-exasol-user>\"              # change\n",
    "EXASOL_PASSWORD = \"exa_pat_<your_password>\"     # change\n",
    "EXASOL_SCHEMA = \"IDA\"                           # change if needed\n",
    "\n",
    "# get the connection\n",
    "EXASOL_CONNECTION = f\"{EXASOL_HOST}:{EXASOL_PORT}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "exasol = pyexasol.connect(dsn=EXASOL_CONNECTION, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Check model file access\n",
    "\n",
    "We will now use a Python UserDefinedFunction (UDF) to check if we can successfully access the uploaded model file in a UDF. You can find the documentation for Exasol UDFs [here](https://docs.exasol.com/saas/database_concepts/udf_scripts.htm).\n",
    "For this we will use a short \"ls\" UDF which takes a path as import and returns a table of filenames found at that path.\n",
    "First we create the UDF in the Exasol Database using our PyExasol connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sql_open_schema = \"\"\"OPEN SCHEMA \"IDA\";\"\"\"\n",
    "sql_ls = \"\"\" --/\n",
    "CREATE OR REPLACE PYTHON3 SCALAR SCRIPT \"IDA.LS\" (\"my_path\" VARCHAR(100)) EMITS (\"FILES\" VARCHAR(100)) AS\n",
    "import os\n",
    "def run(ctx):\n",
    "    for line in os.listdir(ctx.my_path):\n",
    "        ctx.emit(line)\n",
    "/\n",
    "\"\"\"\n",
    "\n",
    "exasol.execute(sql_open_schema)\n",
    "exasol.execute(sql_ls)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then we can call it using the path of our uploaded file as input. It should return a table containing the name of the uploaded model file. If t does not work remember to try replacing the \"bucketfs\" in the front of the path with \"buckets\" instead)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FILES\n",
       "0  model.pkl"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgl_run_ls = \"\"\"SELECT \"IDA.LS\"('/buckets/uploads/default/<your file>');\"\"\"     # change\n",
    "exasol.export_to_pandas(sgl_run_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Call the trained model using a UDF\n",
    "\n",
    "We will now use another [Python UDF](https://docs.exasol.com/saas/database_concepts/udf_scripts.htm) to load the model and use it to classify our test data we loaded into Exasol Saas in the first part of [this tutorial part](ConnectAzureMLtoExasol.ipynb). Take care to only use the [supported](https://docs.exasol.com/saas/database_concepts/udf_scripts/python3.htm) packages, or build your own [Script-Language-Container](https://github.com/exasol/script-languages-container-tool). The UDF takes the data in the provided table and uses the model to classify it. The UDF will then output the result.\n",
    "\n",
    "First we need to create the UDF using our PyExasol connection.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "sql_create_inference_udf = \"\"\"\n",
    "--/\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT IDA.use_model_for_inference(...)\n",
    "EMITS (\"ID\" DECIMAL(20,0), \"prediction\" DOUBLE) AS\n",
    "\n",
    "import urllib.request\n",
    "import lxml.etree as etree\n",
    "import sklearn\n",
    "import numpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_model():\n",
    "    model_path = \"/buckets/uploads/default/testfolder/model.pkl\"  # change to your model file path\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = pickle.load(open(model_path, 'rb'))\n",
    "    return model\n",
    "\n",
    "def infer(data, model):\n",
    "\n",
    "    data = numpy.array(data)\n",
    "    result = model.predict(data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def run(ctx):\n",
    "    model = load_model()\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(num_rows=1000)\n",
    "        if df is None:\n",
    "            break\n",
    "        id_column = df[\"0\"]\n",
    "        df = df.drop(\"0\", 1)\n",
    "        response = infer(df, model)\n",
    "        result = pd.DataFrame(response)\n",
    "        ctx.emit(pd.concat([id_column,result],axis=1))\n",
    "\n",
    "/\n",
    "\"\"\"\n",
    "exasol.execute(sql_create_inference_udf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to select the columns we want to send to the Endpoint. These should be the same we used to train the endpoint in the [training section of the tutorial series](TrainModelInAzureML.ipynb)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['AA_000', 'AG_005', 'AH_000', 'AL_000', 'AM_0', 'AN_000', 'AO_000', 'AP_000', 'AQ_000',\n",
    "                    'AZ_004', 'BA_002', 'BB_000', 'BC_000', 'BD_000', 'BE_000',\n",
    "                    'BF_000', 'BG_000', 'BH_000', 'BI_000', 'BJ_000', 'BS_000', 'BT_000', 'BU_000', 'BV_000',\n",
    "                    'BX_000', 'BY_000', 'BZ_000', 'CA_000', 'CB_000', 'CC_000', 'CI_000', 'CN_004', 'CQ_000',\n",
    "                    'CS_001', 'DD_000', 'DE_000', 'DN_000', 'DS_000', 'DU_000', 'DV_000', 'EB_000', 'EE_005']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now call the UDF. It will predict on our test data directly in Exasol, using the model we trained in AzureML. The call also adds a ROWID to be able to sort the result to the right input. This way we can then check how many where correctly classified by our endpoint."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = exasol.export_to_pandas(\"\"\"SELECT \"CLASS\", \"prediction\" FROM  (\n",
    "                           SELECT IDA.use_model_for_inference(ROWID, {columns_without_class!q}) FROM IDA.TEST t) r\n",
    "                           JOIN IDA.TEST o ON r.ID = o.ROWID\"\"\", {\"columns_without_class\": column_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can create a confusion matrix from our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "predictions      0    1\nactuals                \nneg          14841  784\npos             13  362",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>predictions</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>actuals</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>neg</th>\n      <td>14841</td>\n      <td>784</td>\n    </tr>\n    <tr>\n      <th>pos</th>\n      <td>13</td>\n      <td>362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(index=res['CLASS'], columns=res[\"prediction\"], rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't forget to close your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exasol.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}