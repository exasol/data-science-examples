{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Example Using Exasol\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This example project provides an end-to-end demonstration of how machine learning techniques can be used directly inside Exasol to enable and improve data-driven processes and decision making. We'll use real-world data provided by a heavy truck manufacturer (see [Problem and Data Description](#Problem-Description)) to predict if truck failures are related to a specific component or not. The data is publicly available in the [IDA 2016 Challenge dataset](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) from the Industrial Challenge at the [15th International Symposium on Intelligent Data Analysis](http://ida2016.blogs.dsv.su.se/) (IDA) in 2016.\n",
    "\n",
    "In the process, we demonstrate that there is no need to export data from Exasol to a different computer or server in order to examine or transform the data. Furthermore, it also isn't necessary for training and testing machine learning models. Everything can be done using user-defined functions (UDFs) directly inside Exasol where the data is stored.\n",
    "\n",
    "Because the focus of this example project is on how you can better use machine learning tools with Exasol, we will not discuss machine learning topics such as classifier selection and tuning in depth. Because there are many different machine learning methods, choosing a \"good\" one is highly dependent on the problem to be solved and the data available. Rather, we want to demonstrate how you can more effectively use <b>your models</b> with <b>your data</b> in Exasol.\n",
    "\n",
    "This example project is broken down into the following sections:\n",
    "1. [Problem and Data Description](#Problem-Description)\n",
    "2. [Exasol Setup](#Exasol-Setup)\n",
    "3. [Loading the Data into Exasol](#Loading-the-Data-into-Exasol)\n",
    "4. [Examining the Data](#Examining-the-Data)\n",
    "5. [Transforming the Data](#Transforming-the-Data)\n",
    "6. [Building the Model](#Building-the-Model)\n",
    "7. [Training the Model](#Training-the-Model)\n",
    "8. [Testing the Model](#Testing-the-Model)\n",
    "9. [Evaluating the Model](#Evaluating-the-Model)\n",
    "10. [Deploying the Model](#Deploying-the-Model)\n",
    "11. [Summary](#Summary)\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The intended audience of this article is assumed to have a basic understanding of the following.\n",
    "* Machine learning methods\n",
    "* Exasol, in particular user-defined functions (UDFs)\n",
    "* Python programming, including\n",
    "  * [Scikit-learn](https://scikit-learn.org/stable/)\n",
    "  * [Pandas](https://pandas.pydata.org/)\n",
    "  * [NumPy](http://www.numpy.org/)\n",
    "\n",
    "The following resources might help to understand these topics:\n",
    "  * Python Machine Learning - Second Edition, Sebastian Raschka, Vahid Mirjalili, September 2017\n",
    "  * Learning scikit-learn: Machine Learning in Python, Raúl Garreta, Guillermo Moncecchi, November 2013\n",
    "  * [EXASOL Manual](https://www.exasol.com/portal/display/DOC/User+Manual+6.1.0)\n",
    "### Technical Notes and Recommendations\n",
    "\n",
    "* The code in this Jupyter Notebook was tested using Python 3.6 and 3.7.\n",
    "* We used Exasol 6.0 for this example. If you prefer to use Exasol 6.1, please remember to select the corresponding script-languages flavor (see [Exasol Setup](#Exasol-Setup)).\n",
    "* We recommend that your Exasol instance have at least 6GB RAM.\n",
    "* The code below uses HTTPS to access Exasol's BucketFS by default. If you prefer to use HTTP, please set `EXASOL_BUCKETFS_USE_HTTPS = False` (see [Exasol Setup](#Exasol-Setup)).\n",
    "\n",
    "### Readability Tip\n",
    "\n",
    "The UDF scripts below are defined in triple-quoted Python strings (e.g. `sql = textwrap.dedent(f\"\"\"...\"\"\")`), which prevent the Python syntax highlighting from working in the Jupyter Notebook. If you simply remove one of the quotation marks at the beginning of the string (i.e. `sql = textwrap.dedent(f\"\"...\"\"\")`), the syntax highlighting will work which can greatly improve the code's readability. However, please don't forget to add the quotation mark back before executing the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "This example is based on the publicly available [IDA 2016 Challenge dataset](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) from the Industrial Challenge at the [15th International Symposium on Intelligent Data Analysis](http://ida2016.blogs.dsv.su.se/) (IDA) in 2016.\n",
    "\n",
    "The purpose of the challenge was to best predict which failures were related to a specific component of a truck's air pressure system (APS) as opposed to failures unrelated to the APS. Specifically, the following cost metric was given, which was to be minimized.\n",
    "\n",
    "$cost_{total}=cost_{FP}\\cdot{FP} + cost_{FN}\\cdot{FN}$\n",
    "\n",
    "where  \n",
    "$FP$ is the number of false positives (predicted APS failure, but really isn't),  \n",
    "$FN$ is the number of false negatives (predicted non-APS failure, but really is),  \n",
    "$cost_{FP}=10$ is the cost of an unnecessary check by a mechanic, and  \n",
    "$cost_{FN}=500$ is the cost of not checking a faulty truck and possibly causing a breakdown.\n",
    "\n",
    "From the cost metric, we can see that an unnecessary preventative check is much cheaper (50x) than overlooking a faulty truck, which makes sense.\n",
    "\n",
    "### Data Description\n",
    "\n",
    "The dataset, provided by [Scania CV AB](https://www.scania.com), consists of real data from heavy Scania trucks during normal operation. The following is a brief description of the data. For details, please see the data description file provided with the data.\n",
    "\n",
    "* Number of attributes: 171\n",
    "* Training data:\n",
    "    * Total instances: 60,000\n",
    "    * Positive instances (APS failures): 1000 (1.7% of total)\n",
    "* Test data:\n",
    "    * Total instances: 16,000\n",
    "    * Positive instances (APS failures): 375 (2.3% of total)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Please read the copyright and license information contained in the data files before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exasol Setup\n",
    "\n",
    "Here, we specify some basic information, which is used throughout this example. In particular, we specify the URL, user name, and password for the Exasol host(s) and EXABucket.\n",
    "\n",
    "We also specify the scripting language to be used, which is the 'python3-ds-EXASOL-6.0.0' flavor (i.e. Python 3 with selected data science modules for Exasol 6.0), available in Exasol's [script-languages](https://github.com/exasol/script-languages) GitHub repository. Pre-packaged releases are available in the [release area](https://github.com/exasol/script-languages/releases) of the Github repository. The 'python3-ds-\\*' flavors have the added benefit of integrated [Pandas](https://pandas.pydata.org/) DataFrame support for loading data from Exasol into a script (i.e. `ctx.get_dataframe()`) and emitting data from a script (i.e. `ctx.emit()`). If you use newer version of the 'python3-ds-\\*' flavors (since commit [480d79a](https://github.com/exasol/script-languages/commit/480d79acaf06df789a7a752b956ffcc7969ca596)), you need to change in the EXASOL_UDF_CLIENT from 'exaudfclient' to 'exaudfclient_py3', because since then, Exasol supports Python2 and Python3 UDFs in the same container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined all constants in 924.83μs\n"
     ]
    }
   ],
   "source": [
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "EXASOL_EXTERNAL_HOST_NAME = \"MyCluster_11\"\n",
    "EXASOL_HOST_PORT = \"8888\"\n",
    "EXASOL_EXTERNAL_HOST = f\"\"\"{EXASOL_EXTERNAL_HOST_NAME}:{EXASOL_HOST_PORT}\"\"\"\n",
    "EXASOL_USER = \"sys\"\n",
    "EXASOL_PASSWORD = \"exasol\"\n",
    "EXASOL_BUCKETFS_PORT = \"6583\"\n",
    "EXASOL_EXTERNAL_BUCKETFS_HOST = f\"\"\"{EXASOL_EXTERNAL_HOST_NAME}:{EXASOL_BUCKETFS_PORT}\"\"\"\n",
    "EXASOL_BUCKETFS_USER = \"w\"\n",
    "EXASOL_BUCKETFS_PASSWORD = \"write\"\n",
    "EXASOL_BUCKETFS_USE_HTTPS = False\n",
    "EXASOL_BUCKETFS_URL_PREFIX = \"https://\" if EXASOL_BUCKETFS_USE_HTTPS else \"http://\"\n",
    "EXASOL_BUCKETFS_SERVICE = \"bfsdefault\"\n",
    "EXASOL_BUCKETFS_BUCKET = \"default\"\n",
    "EXASOL_BUCKETFS_PATH = f\"/buckets/{EXASOL_BUCKETFS_SERVICE}/{EXASOL_BUCKETFS_BUCKET}\" # Filesystem-Path to the read-only mounted BucketFS inside the running UDF Container\n",
    "EXASOL_SCRIPT_LANGUAGE_NAME = \"PYTHON3_60\"\n",
    "EXASOL_UDF_FLAVOR = \"python3-ds-EXASOL-6.0.0\"\n",
    "EXASOL_UDF_RELEASE= \"20190116\"\n",
    "EXASOL_UDF_CLIENT = \"exaudfclient\" # or for newer versions of the flavor exaudfclient_py3\n",
    "EXASOL_SCRIPT_LANGUAGES = f\"{EXASOL_SCRIPT_LANGUAGE_NAME}=localzmq+protobuf:///{EXASOL_BUCKETFS_SERVICE}/{EXASOL_BUCKETFS_BUCKET}/{EXASOL_UDF_FLAVOR}?lang=python#buckets/{EXASOL_BUCKETFS_SERVICE}/{EXASOL_BUCKETFS_BUCKET}/{EXASOL_UDF_FLAVOR}/exaudf/{EXASOL_UDF_CLIENT}\";\n",
    "EXASOL_SCHEMA = \"IDA\"\n",
    "\n",
    "connection_params = {\"dns\": EXASOL_EXTERNAL_HOST, \"user\": EXASOL_USER, \"password\": EXASOL_PASSWORD, \"compression\": True}\n",
    "\n",
    "params = {\n",
    "    \"script_languages\": EXASOL_SCRIPT_LANGUAGES,\n",
    "    \"script_language_name\": EXASOL_SCRIPT_LANGUAGE_NAME,\n",
    "    \"schema\": EXASOL_SCHEMA,\n",
    "    \"EXASOL_BUCKETFS_PORT\": EXASOL_BUCKETFS_PORT,\n",
    "    \"EXASOL_BUCKETFS_USER\": EXASOL_BUCKETFS_USER,\n",
    "    \"EXASOL_BUCKETFS_PASSWORD\": EXASOL_BUCKETFS_PASSWORD,\n",
    "    \"EXASOL_BUCKETFS_USE_HTTPS\": EXASOL_BUCKETFS_USE_HTTPS,\n",
    "    \"EXASOL_BUCKETFS_BUCKET\": EXASOL_BUCKETFS_BUCKET,\n",
    "    \"EXASOL_BUCKETFS_PATH\": EXASOL_BUCKETFS_PATH\n",
    "}\n",
    "\n",
    "print(f\"defined all constants in {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need install the python modules, such as pyexasol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyexasol in /opt/conda/lib/python3.7/site-packages (0.6.1)\n",
      "Requirement already satisfied: stopwatch.py in /opt/conda/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (2.21.0)\n",
      "Requirement already satisfied: rsa in /opt/conda/lib/python3.7/site-packages (from pyexasol) (4.0)\n",
      "Requirement already satisfied: websocket-client>=0.47.0 in /opt/conda/lib/python3.7/site-packages (from pyexasol) (0.56.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa->pyexasol) (0.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from websocket-client>=0.47.0->pyexasol) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "#Workarround for notebook to script\n",
    "from IPython import get_ipython\n",
    "if get_ipython() is None:\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.instance()\n",
    "!pip install pyexasol stopwatch.py requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we create an Exasol schema named `IDA`, in which everything will be stored. For this step and throughout the rest of this example project, we use the very convenient [pyexasol](https://github.com/badoo/pyexasol) module, which encapsulates the communication functionality between Python and Exasol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creates Schema in 44.49ms\n"
     ]
    }
   ],
   "source": [
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "# Create schema\n",
    "conn.execute(query=\"CREATE SCHEMA IF NOT EXISTS {schema!i}\", query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "print(f\"creates Schema in {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Further more we need to upload the the script language container to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download: curl -L -o python3-ds-EXASOL-6.0.0.tar.gz  https://github.com/exasol/script-languages/releases/download/20190116/python3-ds-EXASOL-6.0.0-20190116.tar.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   629    0   629    0     0   2082      0 --:--:-- --:--:-- --:--:--  2075\n",
      "100  743M  100  743M    0     0  9156k      0  0:01:23  0:01:23 --:--:-- 12.1M\n",
      "Upload: curl http://w:write@MyCluster_11:6583/default/python3-ds-EXASOL-6.0.0.tar.gz --upload-file python3-ds-EXASOL-6.0.0.tar.gz\n",
      "Finished upload\n",
      "Check Upload: curl http://w:write@MyCluster_11:6583/default/ | grep python3-ds-EXASOL-6.0.0.tar.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   461  100   461    0     0   225k      0 --:--:-- --:--:-- --:--:--  225k\n",
      "python3-ds-EXASOL-6.0.0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "download_command=f\"\"\"curl -L -o {EXASOL_UDF_FLAVOR}.tar.gz  https://github.com/exasol/script-languages/releases/download/{EXASOL_UDF_RELEASE}/{EXASOL_UDF_FLAVOR}-{EXASOL_UDF_RELEASE}.tar.gz\"\"\"\n",
    "print(\"Download: %s\"%download_command)\n",
    "! {download_command}\n",
    "upload_command=f\"\"\"curl {EXASOL_BUCKETFS_URL_PREFIX}{EXASOL_BUCKETFS_USER}:{EXASOL_BUCKETFS_PASSWORD}@{EXASOL_EXTERNAL_BUCKETFS_HOST}/{EXASOL_BUCKETFS_BUCKET}/{EXASOL_UDF_FLAVOR}.tar.gz --upload-file {EXASOL_UDF_FLAVOR}.tar.gz\"\"\"\n",
    "print(\"Upload: %s\"%upload_command)\n",
    "! {upload_command}\n",
    "#wait until script language container got extracted in the BucketFS\n",
    "import time\n",
    "time.sleep(10)\n",
    "print(\"Finished upload\")\n",
    "check_command=f\"\"\"curl {EXASOL_BUCKETFS_URL_PREFIX}{EXASOL_BUCKETFS_USER}:{EXASOL_BUCKETFS_PASSWORD}@{EXASOL_EXTERNAL_BUCKETFS_HOST}/{EXASOL_BUCKETFS_BUCKET}/ | grep {EXASOL_UDF_FLAVOR}.tar.gz\"\"\"\n",
    "print(\"Check Upload: %s\"%check_command)\n",
    "! {check_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an EXABucket Helper Script\n",
    "\n",
    "Before we proceed, we'll create a small helper script in Exasol to define a function, `upload_object_to_bucketfs()`, which simply uploads a Python object to the specified EXABucket so that the object can be loaded and used by Exasol UDFs later. Specifically, we will use this function to save our transformation pipeline and classifier model, which we create below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXABucket Helper Script created in 576.23ms\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", query_params=params)\n",
    "\n",
    "# Create script to upload files to an EXABucket\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT {schema!i}.EXABUCKET_HELPER(...)\n",
    "RETURNS INT AS\n",
    "\n",
    "import os\n",
    "import pycurl\n",
    "import uuid\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Upload object to EXABucket\n",
    "def upload_object_to_bucketfs(object, host, path, user, pw, secure=True):\n",
    "    temp_file = \"/tmp/\" + str(uuid.uuid4().hex + \".pkl\")\n",
    "    joblib.dump(object, temp_file, compress=True)\n",
    "    protocol = 'https' if secure else 'http'\n",
    "\n",
    "    with open(temp_file, \"rb\") as f:\n",
    "        url = protocol + \"://\" + user + \":\" + pw + \"@\" + host + path\n",
    "        curl = pycurl.Curl()\n",
    "        curl.setopt(pycurl.URL, url)\n",
    "        curl.setopt(pycurl.SSL_VERIFYPEER, 0)   \n",
    "        curl.setopt(pycurl.SSL_VERIFYHOST, 0)\n",
    "        curl.setopt(curl.UPLOAD, 1)\n",
    "        curl.setopt(curl.READDATA, f)\n",
    "        curl.perform()\n",
    "        return_code = curl.getinfo(pycurl.RESPONSE_CODE)\n",
    "        if return_code!=200:\n",
    "            raise Exception(\"Upload of %s to %s returned http error %s.\"%(temp_file,url,return_code))\n",
    "        curl.close()\n",
    "\n",
    "    try:\n",
    "        os.remove(temp_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"EXABucket Helper Script created in {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data into Exasol\n",
    "\n",
    "To begin, we download the [IDA 2016 Challenge dataset](https://archive.ics.uci.edu/ml/datasets/IDA2016Challenge) (20MB) from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) and import it into local training and test DataFrames. Because the ZIP file contains a data description file in addition to both the training and test data, which must be kept separate, we cannot import the ZIP file directly into Exasol using Exasol's IMPORT statement. First, we download the ZIP file to our local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the data took: 9.61s\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00414/to_uci.zip\"\n",
    "TRAINING_FILE = \"to_uci/aps_failure_training_set.csv\"\n",
    "TEST_FILE = \"to_uci/aps_failure_test_set.csv\"\n",
    "\n",
    "# Data is preceeded with a 20-line header (copyright & license)\n",
    "NUM_SKIP_ROWS = 20\n",
    "NA_VALUE = \"na\"\n",
    "\n",
    "resp = urlopen(DATA_URL)\n",
    "with open('to_uci.zip', 'wb') as f:  \n",
    "    f.write(resp.read())\n",
    "    \n",
    "print(f\"Downloading the data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we reading the ZIP File into Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data took: 1.97s\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "with ZipFile('to_uci.zip') as z:\n",
    "    with z.open(TRAINING_FILE, \"r\") as f:\n",
    "        train_set = pd.read_csv(f, skiprows=NUM_SKIP_ROWS, na_values=NA_VALUE)\n",
    "    with z.open(TEST_FILE, \"r\") as f:\n",
    "        test_set = pd.read_csv(f, skiprows=NUM_SKIP_ROWS, na_values=NA_VALUE)\n",
    "        \n",
    "print(f\"Reading the data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By having a quick look at the data and/or reading the provided data description file, we can see that the first data column is the class label ('`neg`'/'`pos`') and can be stored in a `VARCHAR(3)` column. The other data columns are all numerical features which can be stored in `DECIMAL(18, 2)` columns. With this information we can now define our column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining column metadata took: 259.74μs\n"
     ]
    }
   ],
   "source": [
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "# Define column names and types\n",
    "column_names = list(train_set.columns)\n",
    "column_types = [\"VARCHAR(3)\"] + [\"DECIMAL(18,2)\"] * (len(column_names) - 1)\n",
    "column_desc = [\" \".join(t) for t in zip(column_names, column_types)]\n",
    "\n",
    "params[\"column_names\"] = column_names\n",
    "params[\"column_desc\"] = column_desc\n",
    "\n",
    "print(f\"Defining column metadata took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the training and test data from the local DataFrames into two tables named `TRAIN` and `TEST`, respectively. At first, we need to define the columns and their types for new tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 60000 rows into TRAIN.\n",
      "Imported 16000 rows into TEST.\n",
      "Importing the data took: 6.71s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "# Create tables for data\n",
    "conn.execute(query=\"CREATE OR REPLACE TABLE {schema!i}.TRAIN(\" + \", \".join(column_desc) + \")\", query_params=params)\n",
    "conn.execute(query=\"CREATE OR REPLACE TABLE {schema!i}.TEST LIKE {schema!i}.TRAIN\", query_params=params)\n",
    "\n",
    "# Import data into Exasol\n",
    "conn.import_from_pandas(train_set, (EXASOL_SCHEMA, \"TRAIN\"))\n",
    "print(f\"Imported {conn.last_statement().rowcount()} rows into TRAIN.\")\n",
    "conn.import_from_pandas(test_set, (EXASOL_SCHEMA, \"TEST\"))\n",
    "print(f\"Imported {conn.last_statement().rowcount()} rows into TEST.\")\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Importing the data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Data\n",
    "\n",
    "After loading the data into Exasol, we may first want to get a feel for the data before creating a classifier. There are many different ways to do so, such as visualizing the data, viewing basic statistical information, examining feature correlation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Data Statistics\n",
    "\n",
    "We will examine the training data's basic statistical information using `pandas.DataFrame.describe()` and `pandas.DataFrame.var()`. The combined results are only shown for the first five columns in order to limit the output for this example, but you can, of course, easily remove this limitation in the `print()` function below to view the statistical information for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CLASS        AA_000        AB_000        AC_000        AD_000\n",
      "count  60000.000000  6.000000e+04  13671.000000  5.666500e+04  4.513900e+04\n",
      "mean       0.016667  5.933650e+04      0.713189  3.560143e+08  1.906206e+05\n",
      "std        0.128020  1.454301e+05      3.478962  7.948749e+08  4.040441e+07\n",
      "min        0.000000  0.000000e+00      0.000000  0.000000e+00  0.000000e+00\n",
      "25%        0.000000  8.340000e+02      0.000000  1.600000e+01  2.400000e+01\n",
      "50%        0.000000  3.077600e+04      0.000000  1.520000e+02  1.260000e+02\n",
      "75%        0.000000  4.866800e+04      0.000000  9.640000e+02  4.300000e+02\n",
      "max        1.000000  2.746564e+06    204.000000  2.130707e+09  8.584298e+09\n",
      "var        0.016389  2.114990e+10     12.103176  6.318261e+17  1.632516e+15\n",
      "Creating statistics for the data took: 38.10s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "#UDF might crash in docker-db with only 6GB RAM without waiting, if you run the cells in quick succession\n",
    "time.sleep(10)\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", query_params=params)\n",
    "\n",
    "# Create script output column descriptions\n",
    "# Numeric data\n",
    "out_column_types = [\"DOUBLE\"] * len(column_names)\n",
    "out_column_desc = [\" \".join(t) for t in zip(column_names, out_column_types)]\n",
    "\n",
    "params[\"out_column_desc\"] = out_column_desc\n",
    "\n",
    "\n",
    "# Create script to run pandas.DataFrame.describe() and pandas.DataFrame.var() in Exasol\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT {schema!i}.DF_DESCRIBE({column_desc!r})\n",
    "EMITS ({out_column_desc!r}) AS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_stats(X):\n",
    "    # Replace 'neg'/'pos' with 0/1\n",
    "    X.loc[:, 'class'] = X.loc[:, 'class'].replace({{'neg': 0, 'pos': 1}})\n",
    "    # Convert all columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    # Get DataFrame stats\n",
    "    X_describe = X.describe()\n",
    "\n",
    "    # Get DataFrame variance\n",
    "    X_var = X.var()\n",
    "    X_var.name = 'var'\n",
    "\n",
    "    # Append variance to stats\n",
    "    return X_describe.append(X_var)\n",
    "\n",
    "def run(ctx):\n",
    "    # Create DataFrame using all columns\n",
    "    df = ctx.get_dataframe(num_rows='all', start_col=0)\n",
    "\n",
    "    # Calculate statistics info\n",
    "    df = get_stats(df)\n",
    "\n",
    "    # Output data description\n",
    "    ctx.emit(df)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Create table \"TRAIN_DESCRIPTION\" to hold the description output\n",
    "\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE TABLE {schema!i}.TRAIN_DESCRIPTION AS\n",
    "    SELECT {schema!i}.DF_DESCRIBE({column_names!r}) FROM {schema!i}.TRAIN\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Create local data frame from the \"TRAIN_DESCRIPTION\" table\n",
    "train_desc = conn.export_to_pandas((EXASOL_SCHEMA,\"TRAIN_DESCRIPTION\"))\n",
    "train_desc.index = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'var']\n",
    "\n",
    "# Print first 5 columns, for example\n",
    "print(train_desc.iloc[:, 0:5])\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Creating statistics for the data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Data\n",
    "\n",
    "Looking at the statistics summary from the previous step, we can see, for example, that some features have missing values and that the means and variances of the features differ greatly. Because of this, it's most likely a good idea to transform, clean and normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Run the Transformation Pipeline\n",
    "\n",
    "Depending on which classifier one plans to use, among other things, there are many different techniques one may use to transform the data, such as feature scaling and extraction.\n",
    "\n",
    "In this example, we first use imputation to replace missing values with the median value of that feature. Then, we scale the data such that each feature is normally distributed (with zero mean and unit variance). These are very simple transformations that work fairly well with many learning algorithms.\n",
    "\n",
    "In the script below, the transformation pipeline is fitted to the training data and used to transform it. Then, the transformer object is saved to the specified EXABucket for future use. Finally, the transformed training data is emitted and stored in the table `TRAIN_TRANSFORMED`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DF_TRANSFORM\n",
      "Creating the transformation pipeline from the train data took: 65.86s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "\n",
    "# Create script output column descriptions\n",
    "# One class label, numeric data\n",
    "out_column_types = [\"INT\"] + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "out_column_desc = [\" \".join(t) for t in zip(column_names, out_column_types)]\n",
    "\n",
    "# File to store the transformer\n",
    "transformer_file = f\"transform_pipeline.pkl\"\n",
    "\n",
    "params[\"out_column_desc\"] = out_column_desc\n",
    "params[\"transformer_file\"] = transformer_file\n",
    "\n",
    "# Create script to transform the data\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT\n",
    "{schema!i}.DF_TRANSFORM(fit_transformer BOOL, transformer_path VARCHAR(200), {column_desc!r})\n",
    "EMITS ({out_column_desc!r}) AS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import helper script\n",
    "exabucket_helper = exa.import_script('IDA.EXABUCKET_HELPER')\n",
    "\n",
    "# Transform DataFrame\n",
    "def transform_dataframe(run_transformer ,X, class_col_name):\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Replace 'neg'/'pos' with 0/1\n",
    "    y = y.replace({{'neg': 0, 'pos': 1}})\n",
    "\n",
    "    # Convert columns to numeric data types\n",
    "    X_data = X_data.apply(pd.to_numeric)\n",
    "\n",
    "    X_data_transformed=run_transformer(X_data)\n",
    "\n",
    "    # Create transformed DataFrame with column names\n",
    "    y_df = pd.DataFrame(y, columns=[class_col_name])\n",
    "    X_data_df = pd.DataFrame(X_data_transformed, columns=X.columns[X.columns != class_col_name])\n",
    "    return y_df.join(X_data_df)\n",
    "\n",
    "def run_fit_transformer(save_path, X_data):\n",
    "        # Fit transformer and transform data\n",
    "    transformer = Pipeline ([\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    X_data_transformed = transformer.fit_transform(X_data)\n",
    "    if save_path:\n",
    "        # Save transformer\n",
    "        exabucket_helper.upload_object_to_bucketfs(transformer,\n",
    "                                                    'localhost:{EXASOL_BUCKETFS_PORT!d}',\n",
    "                                                    save_path,\n",
    "                                                    {EXASOL_BUCKETFS_USER},\n",
    "                                                    {EXASOL_BUCKETFS_PASSWORD},\n",
    "                                                    {EXASOL_BUCKETFS_USE_HTTPS!r})\n",
    "    return X_data_transformed\n",
    "\n",
    "def run_apply_transformer(transformer, X_data):\n",
    "    X_data_transformed = transformer.transform(X_data)\n",
    "    return X_data_transformed\n",
    "    \n",
    "def run(ctx):\n",
    "    # Non-data Input arguments\n",
    "    num_non_data_cols = 2\n",
    "    fit_transformer = ctx.fit_transformer\n",
    "    transformer_path = ctx.transformer_path\n",
    "    \n",
    "    if fit_transformer:\n",
    "        df = ctx.get_dataframe(num_rows='all', start_col=num_non_data_cols)\n",
    "\n",
    "        # Transform feature data\n",
    "        df = transform_dataframe(lambda X: run_fit_transformer(transformer_path, X), \n",
    "                                 df, class_col_name='class')\n",
    "\n",
    "        # Output data\n",
    "        ctx.emit(df)\n",
    "    else:\n",
    "        # Load transformer and transform data\n",
    "        transformer = joblib.load(transformer_path)\n",
    "        # Stream the data through the transformer to reduce the required main memory of the UDF\n",
    "        # which allows to run the UDF on larger datasets and to run multiple UDF containers in parallel\n",
    "        while True:\n",
    "            df = ctx.get_dataframe(num_rows=1000, start_col=num_non_data_cols)\n",
    "            if df is None:\n",
    "                break\n",
    "            df = transform_dataframe(lambda X: run_apply_transformer(transformer, X), \n",
    "                                     df,class_col_name='class')\n",
    "            # Output data\n",
    "            ctx.emit(df)\n",
    "        \n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "print(\"Created DF_TRANSFORM\")\n",
    "\n",
    "# Transform training data\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE TABLE {schema!i}.TRAIN_TRANSFORMED AS\n",
    "    SELECT {schema!i}.DF_TRANSFORM(TRUE, '/{EXASOL_BUCKETFS_BUCKET!i}/{transformer_file!r}', {column_names!i})\n",
    "    FROM {schema!i}.TRAIN\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Creating the transformation pipeline from the train data took: {stopwatch}\")\n",
    "\n",
    "#Wait for distribution of the transformer in the BucketFS\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same script is then called again with the test data. Since the test data should be transformed exactly as the training data, the transformer which was previously saved is simply loaded from the EXABucket and used to transform the test data, which is then emitted and stored in the table `TEST_TRANSFORMED`. Fitting the transformer to the train data required the whole training dataset at once. However, for the application of the fitted tranformer to the test data this is not necessary, as such we can run the transformer in parallel in several instances of the UDF (which is achieved by GROUP BY iproc(), mod(rownum,2)). Furthermore, we stream the test data batch-wise through the transformer, which allows us to transform larger datasets than the limited main memory of the UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the transformation pipeline for the test data took: 9.68s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "# Transform test data\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE TABLE {schema!i}.TEST_TRANSFORMED AS\n",
    "    SELECT {schema!i}.DF_TRANSFORM(FALSE,\n",
    "                            '{EXASOL_BUCKETFS_PATH!r}/{transformer_file!r}',\n",
    "                            {column_names!r})\n",
    "    FROM {schema!i}.TEST\n",
    "    GROUP BY iproc(), mod(rownum,2) -- force the Database to split the data and spawn 2 UDF Container per Node\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "print(f\"Running the transformation pipeline for the test data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model and runing Grid Search to find good Hyper Parameters\n",
    "\n",
    "Now that we've transformed the data, we'll build a model which will be used to predict if each instance is an APS failure or not.\n",
    "\n",
    "For this example, we'll use a classifier based on the Extra-Trees (extremely randomized trees) algorithm. This tree-based ensemble method is similar to a random forest, except that the tree splitting is randomized, among other things. This can improve the accuracy as well as the computation time. Details can be found [here](https://orbi.uliege.be/handle/2268/9357).\n",
    "\n",
    "As with most machine learning algorithms, there are multiple parameters which need to be tuned in order to optimize the performance of the classifier for our problem and data. Rather than try many combinations by hand, we'll use grid search and 5-fold cross validation on the training data to find the optimal parameters in the specified subset of parameters.\n",
    "\n",
    "Because searching a large grid can be computationally intensive, a good set of parameter values has already been found using grid search offline. Thus, only a small, coarse subspace of the search grid is used in the example code below so that executing the script will not take too long.\n",
    "\n",
    "A good set of parameter values found offline using grid search is the following:\n",
    "\n",
    "|Parameter|Value|\n",
    "| :--- | ---: |\n",
    "|n_estimators|61|\n",
    "|max_depth|10|\n",
    "|class_weight|{0: 1, 1: 89}|\n",
    "\n",
    "After the optimal parameter values have been found using grid search, an `ExtraTreesClassifier` model is created using the parameter values and then saved to an EXABucket for use in the next step&mdash;training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridsearch result: [(61, 10, '{0: 1, 1: 89}')]\n",
      "Building the model and Grid search to find good hyper parameters took: 27.44s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "# Script input column descriptions are now the same as output\n",
    "# One class label, numeric data\n",
    "column_types = [\"INT\"] + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "column_desc = [\" \".join(t) for t in zip(column_names, column_types)]\n",
    "\n",
    "params[\"column_desc\"] = column_desc\n",
    "params[\"classifier_file\"] = \"classifier.pkl\"\n",
    "\n",
    "# Create script to build the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT\n",
    "{schema!i}.BUILD_MODEL(classifier_path VARCHAR(200), {column_desc!r})\n",
    "EMITS (n_estimators int, max_depth int, class_weight VARCHAR(200)) AS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import helper script\n",
    "exabucket_helper = exa.import_script('{schema!i}.EXABUCKET_HELPER')\n",
    "\n",
    "# Random state to use for reproducibility\n",
    "RAND_STATE = 3\n",
    "\n",
    "# Build extra-tree classifier\n",
    "def build_et_classifier(X, class_col_name, model_path=None):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Create classifier\n",
    "    clf = ExtraTreesClassifier(random_state=RAND_STATE, n_jobs=-1)\n",
    "\n",
    "    # Specify parameter search grid\n",
    "    # The grid size is kept small to reduce the computation time\n",
    "    # Good values (known from offline grid search) are:\n",
    "    # 'n_estimators': 61\n",
    "    # 'max_depth': 10\n",
    "    # 'class_weight': {{0: 1, 1: 89}}\n",
    "    param_grid = [{{\n",
    "        'n_estimators': [30, 61],\n",
    "        'max_depth': [5, 10],\n",
    "        'class_weight': [{{0: 1, 1: 89}}]\n",
    "    }}]\n",
    "\n",
    "    # Define scoring metric for grid search from problem description\n",
    "    def ida_score(y, y_pred):\n",
    "        false_preds = y - y_pred\n",
    "        num_false_pos = (false_preds < 0).sum()\n",
    "        num_false_neg = (false_preds > 0).sum()\n",
    "        return -(num_false_pos * 10 + num_false_neg * 500)\n",
    "\n",
    "    ida_scorer = make_scorer(ida_score)\n",
    "\n",
    "    # Search for optimal values in grid using 5-fold cross validation\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=ida_scorer, n_jobs=-1)\n",
    "    grid_search.fit(X_data, y.values.ravel())\n",
    "\n",
    "    # Create new model with optimal parameter values\n",
    "    clf = ExtraTreesClassifier(random_state=RAND_STATE, n_jobs=-1,\n",
    "                                n_estimators=grid_search.best_params_['n_estimators'],\n",
    "                                max_depth=grid_search.best_params_['max_depth'], \n",
    "                                class_weight=grid_search.best_params_['class_weight'])\n",
    "\n",
    "    # Save classifier to EXABucket\n",
    "    if model_path:\n",
    "        exabucket_helper.upload_object_to_bucketfs(clf,\n",
    "                                                    'localhost:{EXASOL_BUCKETFS_PORT!r}',\n",
    "                                                    model_path,\n",
    "                                                    {EXASOL_BUCKETFS_USER},\n",
    "                                                    {EXASOL_BUCKETFS_PASSWORD},\n",
    "                                                    {EXASOL_BUCKETFS_USE_HTTPS!r})\n",
    "    return grid_search\n",
    "\n",
    "def run(ctx):\n",
    "    # Input argument\n",
    "    num_non_data_cols = 1\n",
    "    classifier_path = ctx.classifier_path\n",
    "\n",
    "    df = ctx.get_dataframe(num_rows='all', start_col=num_non_data_cols)\n",
    "\n",
    "    # Shuffle data\n",
    "    train_set = resample(df, n_samples=30000, replace=False, random_state=RAND_STATE)\n",
    "\n",
    "    # Build extra-tree classifier\n",
    "    grid_search=build_et_classifier(train_set, class_col_name='class', model_path=classifier_path)\n",
    "    ctx.emit(grid_search.best_params_['n_estimators'],\n",
    "              grid_search.best_params_['max_depth'],\n",
    "              str(grid_search.best_params_['class_weight']))\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Build model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "SELECT {schema!i}.BUILD_MODEL('/{EXASOL_BUCKETFS_BUCKET!r}/{classifier_file!r}', {column_names!r})\n",
    "FROM {schema!i}.TRAIN_TRANSFORMED\n",
    "\"\"\")\n",
    "\n",
    "print(\"Gridsearch result:\",conn.execute(query=sql, query_params=params).fetchall())\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Building the model and Grid search to find good hyper parameters took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "After transforming the data and creating the classifier, we'll now train it on all the transformed training data. Then, the model will be stored in the provided EXABucket for later use during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model took: 16.66s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "# Create script to train the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT\n",
    "{schema!i}.TRAIN_MODEL(classifier_load_path VARCHAR(200), classifier_save_path VARCHAR(200), {column_desc!r})\n",
    "EMITS (dummy int) AS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Import helper script\n",
    "exabucket_helper = exa.import_script('{schema!i}.EXABUCKET_HELPER')\n",
    "\n",
    "# Random state to use for reproducibility\n",
    "RAND_STATE = 3\n",
    "\n",
    "# Train classifier\n",
    "def train(X, class_col_name, model_load_path, model_save_path):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    y = X.loc[:, class_col_name]\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "\n",
    "    # Load model from EXABucket\n",
    "    clf = joblib.load(model_load_path)\n",
    "    clf.fit(X_data, y.values.ravel())\n",
    "\n",
    "    # Save classifier to EXABucket\n",
    "    if model_save_path:\n",
    "        exabucket_helper.upload_object_to_bucketfs(clf,\n",
    "                                                    'localhost:{EXASOL_BUCKETFS_PORT!r}',\n",
    "                                                    model_save_path,\n",
    "                                                    {EXASOL_BUCKETFS_USER},\n",
    "                                                    {EXASOL_BUCKETFS_PASSWORD},\n",
    "                                                    {EXASOL_BUCKETFS_USE_HTTPS!r})\n",
    "\n",
    "def run(ctx):\n",
    "    # Input arguments\n",
    "    num_non_data_cols = 2\n",
    "    classifier_load_path = ctx.classifier_load_path\n",
    "    classifier_save_path = ctx.classifier_save_path\n",
    "\n",
    "    df = ctx.get_dataframe(num_rows='all', start_col=num_non_data_cols)\n",
    "\n",
    "    # Shuffle data\n",
    "    train_set = resample(df, replace=False, random_state=RAND_STATE)\n",
    "\n",
    "    # Train the classifier\n",
    "    train(train_set,\n",
    "          class_col_name='class',\n",
    "          model_load_path=classifier_load_path,\n",
    "          model_save_path=classifier_save_path)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Train model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "SELECT IDA.TRAIN_MODEL('{EXASOL_BUCKETFS_PATH!r}/{classifier_file!r}',\n",
    "                        '/{EXASOL_BUCKETFS_BUCKET!r}/{classifier_file!r}',\n",
    "                        {column_names!r})\n",
    "FROM IDA.TRAIN_TRANSFORMED\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Training the model took: {stopwatch}\")\n",
    "\n",
    "\n",
    "#Wait for distribution of the classifier in the BucketFS\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "After training the classifier, we'll now test it using the transformed test data.\n",
    "\n",
    "The model that was saved after training will now be loaded from the EXABucket and used to predict the classes of the test data (i.e. whether a failure is an APS failure or not). The emitted results, which are stored in the table `TEST_PREDICTIONS`, are the predicted classes (first column) joined to the transformed test data. By joining the predicted class labels to the test data, we ensure that the predicted and real class labels remain properly ordered/linked for evaluation.\n",
    "Similar to the transformer, we needed the whole dataset for the training of the classifer. However, for the application of the fitted classifier to the test data this is not necessary, as such we can run the classifier in parallel in several instances of the UDF (which is achieved by GROUP BY iproc(), mod(rownum,2)). Furthermore, we stream the test data batch-wise through the transformer, which allows us to classify larger datasets than the limited main memory of the UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the model took: 6.60s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "# Create script output column descriptions\n",
    "# Two class labels, numeric data\n",
    "out_column_types = [\"INT\"] * 2 + [\"DOUBLE\"] * (len(column_names) - 1)\n",
    "out_column_desc = [\" \".join(t) for t in zip([\"class_pred\"] + column_names, out_column_types)]\n",
    "\n",
    "params[\"out_column_desc\"] = out_column_desc\n",
    "\n",
    "# Create script to test the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT\n",
    "{schema!i}.TEST_MODEL(classifier_path VARCHAR(200), {column_desc!r})\n",
    "EMITS ({out_column_desc!r}) AS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Test classifier\n",
    "def test(X, class_col_name, clf):\n",
    "    # Convert columns to numeric data types\n",
    "    X = X.apply(pd.to_numeric)\n",
    "\n",
    "    X_data = X.loc[:, X.columns != class_col_name]\n",
    "    \n",
    "    # Predict classes of test data\n",
    "    return clf.predict(X_data)\n",
    "    \n",
    "def run(ctx):\n",
    "    # Input argument\n",
    "    num_non_data_cols = 1\n",
    "    classifier_path = ctx.classifier_path\n",
    "    # Load model from EXABucket\n",
    "    num_non_data_cols = 1\n",
    "    classifier_path = ctx.classifier_path\n",
    "    # Load model from EXABucket\n",
    "    clf = joblib.load(classifier_path)\n",
    "    # Stream the data through the model to reduce the required main memory of the UDF\n",
    "    # which allows to run the UDF on larger datasets and to run multiple UDF containers in parallel\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(num_rows=1000, start_col=num_non_data_cols)\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        # Test the classifier\n",
    "        y_pred = test(df, class_col_name='class', clf=clf)\n",
    "\n",
    "        # Add class predictions as first column of test DataFrame\n",
    "        df_pred = (pd.DataFrame(y_pred, columns=['class_pred'])).join(df)\n",
    "\n",
    "        # Convert columns to numeric data types\n",
    "        df_pred = df_pred.apply(pd.to_numeric)\n",
    "\n",
    "        # Output data\n",
    "        ctx.emit(df_pred)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Test model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE TABLE {schema!i}.TEST_PREDICTIONS AS\n",
    "    SELECT {schema!i}.TEST_MODEL('{EXASOL_BUCKETFS_PATH!r}/{classifier_file!r}', {column_names!r})\n",
    "    FROM {schema!i}.TEST_TRANSFORMED\n",
    "    GROUP BY iproc(), mod(rownum,2) -- force the Database to split the data and spawn 2 UDF Container per Node\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Test the model took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that we have the predicted class labels of the test data, we can simply compare them to the actual class labels to evaluate how well the classifier performed.\n",
    "\n",
    "For the performance metric, we use the `ida_cost()` method defined below, which implements the cost function specified in the problem description. Additionally, the confusion matrix is also displayed to see how the instances were classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost: 10590 \n",
      "\n",
      "Confusion Matrix:\n",
      "             predicted neg  predicted pos\n",
      "actual neg          14866            759\n",
      "actual pos              6            369\n",
      "Evaluating the model took: 1.26s\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Define cost function from the problem description\n",
    "def ida_cost(y, y_pred):\n",
    "    false_preds = y - y_pred\n",
    "    num_false_pos = (false_preds < 0).sum()\n",
    "    num_false_neg = (false_preds > 0).sum()\n",
    "    return 10 * num_false_pos + 500 * num_false_neg\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "\n",
    "# Get predicted and real class labels\n",
    "test_preds = conn.export_to_pandas(query_or_table=\"SELECT CLASS_PRED, CLASS FROM {schema!i}.TEST_PREDICTIONS\", query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "y_pred = test_preds.loc[:, 'CLASS_PRED']\n",
    "y = test_preds.loc[:, 'CLASS']\n",
    "\n",
    "# Examine the results\n",
    "confusion_mat = confusion_matrix(y, y_pred)\n",
    "confusion_matrix_df = pd.DataFrame(confusion_mat,\n",
    "                                   index=['actual neg', 'actual pos'],\n",
    "                                   columns=['predicted neg', 'predicted pos'])\n",
    "\n",
    "print(\"Total Cost:\", ida_cost(y, y_pred),\"\\n\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_df)\n",
    "\n",
    "print(f\"Evaluating the model took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the evaluation script above, the following results (or values very similar to them&mdash;possibly depending on the system) should be displayed.\n",
    "\n",
    "***\n",
    "\n",
    "<b>Total Cost:</b> 10590\n",
    "\n",
    "<b>Confusion Matrix:</b>\n",
    "\n",
    "|&nbsp;|<b>predicted neg</b>|<b>predicted pos</b>|\n",
    "|---------|------------|------------|\n",
    "|<b>actual neg</b>|14866|759|\n",
    "|<b>actual pos</b>|    6|369|\n",
    "\n",
    "***\n",
    "\n",
    "While there are many different methods for evaluating a model, we are interested in minimizing the total cost as described in the problem description. By looking at the confusion matrix, we can see that the total cost was calculated as $10\\cdot{759} + 500\\cdot{6}=10590$.\n",
    "\n",
    "Note that since the costs for false negatives and false positives are not equal, this model may not necessarily have the highest classification accuracy. This makes sense since false negatives are punished much more severely (50x) than false positives. So intuitively, the model would much rather err classifying a negative as positive than a positive as a negative. And when we look at the confusion matrix and some other performance metrics, we see that this is indeed the case.\n",
    "\n",
    "The <b>classification accuracy</b>, which is the ratio of correct predictions to total predictions, is $\\frac{14866+369}{16000}=0.95$. However, this value is not the most relevant in this case as mentioned above. We are much more interested in minimizing false negatives than false positives.\n",
    "\n",
    "Similarly, if we look at the <b>precision</b> metric for the classifier, which is the ratio of true positives to true predictions, $\\frac{369}{369+759}=0.33$ does not seem to be too good. In fact, the model has predicted over 2x as many false positives as true positives. However because the false positives have a relatively low cost, the performance is perhaps not as bad as it seems.\n",
    "\n",
    "On the other hand, because false negatives are so expensive, the <b>recall</b> (or true positive rate) metric is more telling in our case. The recall, which is the ratio of true positives to actual positives, has a value of $\\frac{369}{369+6}=0.98$, which means that the model correctly classified over 98% of all actual positives.\n",
    "\n",
    "If we switch back to the terminology of the problem, the performance of the classifier can be summarized as the following.\n",
    "* 98% of APS failures are correctly identified and the trucks are properly checked.\n",
    "* 2% of trucks with a faulty APS are not properly checked resulting in a potential breakdown.\n",
    "* 67% of APS checks are unnecessary because the trucks do not have a faulty APS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model\n",
    "\n",
    "After evaluating the model and deciding that it is ready for production, all we need to do is deploy it. This is quite simple since we just need to copy the model we previously evaluated to an EXABucket of a production Exasol cluster, where it can be used with live data.\n",
    "\n",
    "In the short script below, we upload the evaluated model to a new EXABucket location. Note: In order to keep this example simple, the model is simply uploaded to a different path on the same Exasol cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying the model took: 699.05ms\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create Exasol connection\n",
    "conn = pyexasol.connect(dsn=EXASOL_EXTERNAL_HOST, user=EXASOL_USER, password=EXASOL_PASSWORD, compression=True)\n",
    "conn.execute(query=\"ALTER SESSION SET SCRIPT_LANGUAGES={script_languages}\", \n",
    "             query_params=params)\n",
    "\n",
    "# File to store the production classifier\n",
    "production_file = \"production_classifier.pkl\"\n",
    "\n",
    "params[\"production_file\"] = production_file\n",
    "\n",
    "# Create script to deploy the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE {script_language_name!i} SET SCRIPT\n",
    "{schema!i}.DEPLOY_MODEL(classifier_load_path VARCHAR(200), classifier_production_path VARCHAR(200))\n",
    "EMITS (dummy int) AS\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Import helper script\n",
    "exabucket_helper = exa.import_script('{schema!i}.EXABUCKET_HELPER')\n",
    "\n",
    "def run(ctx):\n",
    "    # Load model from EXABucket\n",
    "    clf = joblib.load(ctx.classifier_load_path)\n",
    "\n",
    "    # Save classifier to EXABucket\n",
    "    exabucket_helper.upload_object_to_bucketfs(clf,\n",
    "                                                'localhost:{EXASOL_BUCKETFS_PORT!r}',\n",
    "                                                ctx.classifier_production_path,\n",
    "                                                {EXASOL_BUCKETFS_USER},\n",
    "                                                {EXASOL_BUCKETFS_PASSWORD},\n",
    "                                                {EXASOL_BUCKETFS_USE_HTTPS!r})\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Deploy model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "SELECT IDA.DEPLOY_MODEL('{EXASOL_BUCKETFS_PATH!r}/{classifier_file!r}', '/{EXASOL_BUCKETFS_BUCKET!r}/{production_file!r}')\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(query=sql, query_params=params)\n",
    "\n",
    "# Close Exasol connection\n",
    "conn.close()\n",
    "\n",
    "print(f\"Deploying the model took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this small example project, we went through each of the main steps of a machine learning project while using a real-world, industrial problem and data as an example. We started from the very beginning by downloading the data and finished with a production-deployed machine learning model ready to use for making intelligent, data-driven business decisions.\n",
    "\n",
    "We demonstrated that each step in the process can be completed using Exasol's UDFs, so there isn't a need to separate the database from machine learning methods anymore. There's no need to export the data to a separate machine or server in order to analyze it and build and train machine learning models based on it. You can build, train, and test your models by accessing the data directly from inside the database. You can do it all in one place &mdash; Exasol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
